{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Like other data types, text data never comes clean. Moreover, most of our downstream methods only accept data structured in a particular way. Because of this, before we do any computational text analysis techniques, we will always need to perform some level of preprocessing. Text data has its own unique kind of preprocessing. In this notebook, we will cover the core preprocessing methods to get your feet wet:\n",
    "\n",
    "- Reading in .txt and .csv files\n",
    "- Tokenization\n",
    "- Sentence segmentation\n",
    "- Removing punctuation\n",
    "- Stripping whitespace\n",
    "- Text normalization\n",
    "- Stop words\n",
    "- Stemming/Lemmatizing\n",
    "- POS tagging\n",
    "\n",
    "This notebook assumes you have basic familiarity with Python. If you need a beginner's introduction to Python, see the notebook at `solutions/intro-to-python.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in files\n",
    "\n",
    "The first step is to read in the files containing the data. The most common file types for text data are: `.txt`, `.csv`, `.json`, `.html` and `.xml`.\n",
    "\n",
    "### Reading in `.txt` files\n",
    "\n",
    "Python has built-in support for reading in `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_DIR = 'data'\n",
    "fname = 'pride-and-prejudice.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review of Python string methods\n",
    "\n",
    "- What type of object is `raw`?\n",
    "- How many characters are in `raw`?\n",
    "- Get the first 1000 characters of `raw`?\n",
    "- Join together the first 200 and the last 200 characters of `raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in `.csv`\n",
    "\n",
    "Python has a built-in module called `csv` for reading in csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fname = 'trump-tweets.csv'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "tweets = []\n",
    "#with open(fname) as f:\n",
    "import codecs\n",
    "with codecs.open(fname, \"r\", encoding='utf-8', errors='ignore') as f: ##for special encoding issues  \n",
    "    reader = csv.reader(f)\n",
    "    tweets = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review of Python list methods\n",
    "\n",
    "- What data type is `tweets`?\n",
    "- How many entries are in `tweets`?\n",
    "- Which entry is the header row?\n",
    "- Get the first 10 entries.\n",
    "- Join together the 5th and 10th elements of `tweets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in `.csv` with `pandas`\n",
    "\n",
    "`pandas` is a third-party library that makes working with tabular data much easier. This is the recommended way to read in a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "fname = 'trump-tweets.csv'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "tweets = pd.read_csv(fname) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review of `pandas`\n",
    "\n",
    "- What data type is `tweets`?\n",
    "- How many tweets are there?\n",
    "- What happened to the header row?\n",
    "- Get the first row of `tweets`.\n",
    "- Get the first 5 entries in the `Tweet_Text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in multiple files\n",
    "\n",
    "Often, our text data is split across multiple files in a folder. We want to read them all into a single variable. <br>`glob` is a handy package for this: it lists all files matching a pattern. We can use this to get all files in a folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "fnames = os.path.join(DATA_DIR, 'austen', '*.txt')\n",
    "fnames = glob.glob(fnames)\n",
    "austen = ''\n",
    "for fname in fnames:\n",
    "    with codecs.open(fname, \"r\", encoding='utf-8-sig', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "        austen += text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review of working with files\n",
    "\n",
    "- What does `os.path.join()` do in this case?\n",
    "- What type is `fnames` after it is first assigned a value?\n",
    "- What type is `fnames` after it is assigned a second value?\n",
    "- How many files are in `fnames`?\n",
    "- What type is `austen`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Read in all the `.csv` files in the folder `amazon`. Extract out only the `text` column from THE FIRST TWO files and store them all in a list called `reviews`. \n",
    "\n",
    "**Hint 1:** Not all of these files heave a header row to indicate column names. But for your reference, the columns are in this order: <br>\n",
    "```Id, ProductId, UserId, ProfileName, HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary, Text```\n",
    "\n",
    "**Hint 2:** You can deal with `.csv` files without header rows by calling the argument `header=None` when loading into a pandas DataFrame. This lets pandas know not to mistake the first row of data for column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20000</td>\n",
       "      <td>B002C50X1M</td>\n",
       "      <td>A3LWC833HQIG7J</td>\n",
       "      <td>austin_Larry</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1295568000</td>\n",
       "      <td>Excellent chips, full of flavor and just the r...</td>\n",
       "      <td>I purchased the Salt and Vinegar chips and hav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20001</td>\n",
       "      <td>B002C50X1M</td>\n",
       "      <td>AJKQOYD9ILG9Y</td>\n",
       "      <td>T. Ferek</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1295049600</td>\n",
       "      <td>good chips</td>\n",
       "      <td>This is a heavy kettle style chip that is not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20002</td>\n",
       "      <td>B002C50X1M</td>\n",
       "      <td>A2WV4N0X29CIFN</td>\n",
       "      <td>leecash2fly</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1294099200</td>\n",
       "      <td>Great Chips</td>\n",
       "      <td>I recently bought these chips.  Came home and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20003</td>\n",
       "      <td>B002C50X1M</td>\n",
       "      <td>A32SQ3PVLO0UGQ</td>\n",
       "      <td>J. Gilbert</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1293753600</td>\n",
       "      <td>Very good taste-twist chips</td>\n",
       "      <td>These are not the \"same old\" chips by far and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20004</td>\n",
       "      <td>B002C50X1M</td>\n",
       "      <td>A1WRBCQDIH209I</td>\n",
       "      <td>angiechildress</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1286064000</td>\n",
       "      <td>chips</td>\n",
       "      <td>I love Rosemary chips, but can't find any at m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0           1               2               3  4  5  6           7  \\\n",
       "0  20000  B002C50X1M  A3LWC833HQIG7J    austin_Larry  0  0  5  1295568000   \n",
       "1  20001  B002C50X1M   AJKQOYD9ILG9Y        T. Ferek  0  0  5  1295049600   \n",
       "2  20002  B002C50X1M  A2WV4N0X29CIFN     leecash2fly  0  0  5  1294099200   \n",
       "3  20003  B002C50X1M  A32SQ3PVLO0UGQ      J. Gilbert  0  0  4  1293753600   \n",
       "4  20004  B002C50X1M  A1WRBCQDIH209I  angiechildress  0  0  5  1286064000   \n",
       "\n",
       "                                                   8  \\\n",
       "0  Excellent chips, full of flavor and just the r...   \n",
       "1                                         good chips   \n",
       "2                                        Great Chips   \n",
       "3                        Very good taste-twist chips   \n",
       "4                                              chips   \n",
       "\n",
       "                                                   9  \n",
       "0  I purchased the Salt and Vinegar chips and hav...  \n",
       "1  This is a heavy kettle style chip that is not ...  \n",
       "2  I recently bought these chips.  Came home and ...  \n",
       "3  These are not the \"same old\" chips by far and ...  \n",
       "4  I love Rosemary chips, but can't find any at m...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       I purchased the Salt and Vinegar chips and hav...\n",
       "1       This is a heavy kettle style chip that is not ...\n",
       "2       I recently bought these chips.  Came home and ...\n",
       "3       These are not the \"same old\" chips by far and ...\n",
       "4       I love Rosemary chips, but can't find any at m...\n",
       "                              ...                        \n",
       "9995    I really like Pamela's baking mix so I tried t...\n",
       "9996    This is the best gf bread mix I have found by ...\n",
       "9997    THIS BREAD MIX IS THE CLOSEST THING TO REGULAR...\n",
       "9998    Delicious and easy to make.  An excellent brea...\n",
       "9999    I bought this mix for my daughter's boyfriend,...\n",
       "Name: 9, Length: 10000, dtype: object"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amz_df1[9] # This is the `text` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I purchased the Salt and Vinegar chips and have been very pleased. There is the right amount of vinegar, virtually every single chip I have tasted is done just right, no burned chips, and they have an excellent thickness to impart just the correct amount of potato taste.<br /><br />They go great with lunches or as a snack. They are very economical. Beats the heck in terms of quality, taste, and price to buying these at work or out and about (I bought the 2 ounce bags). I will be trying some of the other intriguing flavors. Recommended.',\n",
       " 'This is a heavy kettle style chip that is not as \"heavy\" as kettle brand chips, but bas good flavor.  I will reorder.',\n",
       " \"I recently bought these chips.  Came home and only three bags were left in the box.  My whole family loves them.  They have great flavor and better and healthier than more chips. I'm ordering a few more boxes today\",\n",
       " 'These are not the \"same old\" chips by far and the Asian Sweet & Spicy sparked my imagination too, makes you appreciate the new and improved variety of potatoe chips. This flavor is easily an \"8\" on the ten-scale.',\n",
       " \"I love Rosemary chips, but can't find any at my location. I was glad I came across these, at such a great price.\",\n",
       " \"The potato chips are wonderfully tasty.  My sons and I both like the Rosemary and Olive Oil because it is not as spicy.  We also got the Jalapeno.  It is not so much I don't like the Jalapeno.  It is spicy hot for me.  I still love the potato chips.\",\n",
       " 'Being what you would call a chip connoisseur was looking forward to trying these.  Great chips.',\n",
       " 'Very nice chips; just the right size to serve with a sandwich and watch my calories.',\n",
       " 'My order was carried out in a fast and effective manner. I received my shipment of chips within two days and had no problems with the retailer.<br /><br />The chips (Sweet Onion variety) are flavorful and particularly crunchy. The only negative--a subjective one--is that the chips may be too salty. Other than that, I would recommend buying these chips again.',\n",
       " 'This is a re-post of the review I left for the 5 oz bags ... Addictive and totally unexpected. I first tasted these at the Fancy Food Show in New York a couple of years ago. Was amazed to find them on Amazon - and bought immediately for our lunch clients. What you want in a potato chip: the crunch is fantastic and the flavor gives off both rosemary and olive oil, ending in a surprisingly smoky note.']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = list(amz_df1[9]) + list(amz_df2[9])\n",
    "reviews[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Once we've read in the data, our next step is often to split it into words. This step is referred to as \"tokenization\". That's because each occurrence of a word is called a \"token\". Each distinct word used is called a word \"type\". So the word type \"the\" may correspond to multiple tokens of \"the\" in a text.\n",
    "\n",
    "#### Tokenizing by whitespace\n",
    "\n",
    "- What problems do you notice with tokenizing by whitespace?\n",
    "- What type is `text`?\n",
    "- What type is `tokens`?\n",
    "- What type is each element of `tokens`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fname = 'example1.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing with regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "word_pattern = r'\\w+'\n",
    "tokens = re.findall(word_pattern, text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing with `nltk`\n",
    "\n",
    "[Just a bunch of regular expressions under the hood](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/treebank.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk; nltk.download('punkt')\n",
    "tokens = word_tokenize(text)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "A while ago you read in a bunch of Jane Austen books into a variable called `austen`. Tokenize that using a method of your choice. Find all the unique words types (you might want the `set` function). Sort the resulting set object to create a vocabulary (you might want to use the `sorted` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation\n",
    "\n",
    "Sentence segmentation involves identifying the boundaries of sentences.\n",
    "\n",
    "#### Sentence segmentation by splitting on punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could improve on this by using regular expressions. They'll allow us to split strings based on a number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_boundary_pattern = r'[.?!]'\n",
    "re.split(sent_boundary_pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "The file `example2.txt` has more punctuation problems. Read it in and see what the problems are. Try your best to modify the code from above to work for as many cases as you can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence segmentation by `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "fname = 'example2.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuation\n",
    "\n",
    "Sometimes (although admittedly less frequently than tokenizing and sentence segmentation), you might want to keep only the alphanumeric characters (i.e. the letters and numbers) and ditch the punctuation. Here's how we can do that.\n",
    "\n",
    "- What type is `punctuation`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punct = ''.join([ch for ch in text if ch not in punctuation])\n",
    "no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip whitespace\n",
    "\n",
    "This is an extremely common step. It's simple to perform and nicely pre-packaged in Python. It's particularly common for user-generated text (think survey forms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ' Hello! '\n",
    "string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'example3.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_text = text.strip()\n",
    "print(stripped_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whitespace_pattern = r'\\s+'\n",
    "clean_text = re.sub(whitespace_pattern, ' ', text)\n",
    "clean_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalization\n",
    "\n",
    "Text normalization means making our text fit some standard patterns. Lots of steps come under this wide umbrella, but the most common are:\n",
    "\n",
    "- case folding\n",
    "- removing URLs, digits, hashtags\n",
    "- OOV (removing infequent words) (not done here)\n",
    "\n",
    "#### Case folding\n",
    "\n",
    "Case folding means dealing with upper and lower cases characters. This is usually done by making all characters lower cased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'example4.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "The `lower` method we used above is a string method, that is, it works on strings. But what if you want to lowercase every word in a list (say you've already tokenized the text). Take the list of tokens below and make each one lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs, digits and hashtags\n",
    "\n",
    "We rarely care about the exact URL used in a tweet, or the exact number. We could remove them completely (think about how we'd do that), but it's often informative to know that there is a URL or a digit in the text. So we want to replace individual URLs asnd digits with a symbol that preserves the fact that a URL was there. It's standard to just use the strings \"URL\" and \"DIGIT\".\n",
    "\n",
    "How do we do this? Once again, regular expressions save the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "single_tweet = tweet_text[0]\n",
    "single_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_SIGN = ' URL '\n",
    "re.sub(url_pattern, URL_SIGN, single_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we replaced the URL in a single tweet. Now we will replace all the URLs in all tweets in `tweet_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "URL_SIGN = ' URL '\n",
    "list_of_url_less_tweets = []\n",
    "## Using a for loop\n",
    "for tweet in tweet_text:\n",
    "    url_less_tweet = re.sub(url_pattern, URL_SIGN, tweet)\n",
    "    list_of_url_less_tweets.append(url_less_tweet)\n",
    "list_of_url_less_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternative using list comprehension\n",
    "list_of_url_less_tweets = [re.sub(url_pattern, URL_SIGN, tweet) for tweet in tweet_text]\n",
    "list_of_url_less_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove hashtags and digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\n",
    "HASHTAG_SIGN = ' HASHTAG '\n",
    "digit_pattern = '\\d+'\n",
    "DIGIT_SIGN = ' DIGIT '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_hashtags = [re.sub(hashtag_pattern, HASHTAG_SIGN, tweet) for tweet in tweet_text]\n",
    "no_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_digit = [re.sub(digit_pattern, DIGIT_SIGN, tweet) for tweet in tweet_text]\n",
    "no_digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting word frequencies (after text normalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the frequency of each word type with the built-in `Counter` in Python. This basically just takes the set of word types (we calculated this above as `vocabulary`) and makes a special Python dictionary with each value being the number of times it appears in the list. We can ask that dictionary for the most common words, or for the frequency of individual word types. \n",
    "\n",
    "First, clean and normalize the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = ' '.join(tweets)\n",
    "clean = re.sub(url_pattern, URL_SIGN, all_tweets)\n",
    "clean = re.sub(hashtag_pattern, HASHTAG_SIGN, clean)\n",
    "clean = re.sub(digit_pattern, DIGIT_SIGN, clean)\n",
    "tokens = word_tokenize(clean)\n",
    "tokens = [token for token in tokens if token not in punctuation]\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "freq = Counter(tokens)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "I've read in some Amazon reviews from earlier into a list called `reviews`. Each element of the list is a string, representing the text of a single review. Try to:\n",
    "- Tokenize each review\n",
    "- Strip all whitespace\n",
    "- Make all characters lower case\n",
    "- Replace any URLs and digits\n",
    "\n",
    "Then find the most common 50 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words\n",
    "\n",
    "You might have noticed that the most common words above aren't terribly exciting. They're words like \"am\", \"i\", \"the\" and \"a\": stop words. These are rarely useful to us in computational text analysis, so it's very common to remove them completely.\n",
    "\n",
    "- What other stop words do you think there are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Use the list `stop` of English stopwords to remove stopwords from our tokenized review above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming/lemmatization\n",
    "\n",
    "Stemming and lemmatization both refer to remove morphological affixes on words. For example, if we stem the word \"grows\", we get \"grow\". If we stem the word \"running\", we get \"run\". We do this because often we care more about the core content of the word (i.e. that it has something to do with growth or running, rather than the fact that it's a third person present tense verb, or progressive participle).\n",
    "\n",
    "NLTK provides many algorithms for stemming. For English, a great baseline is the [Porter](https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py) algorithm, which is in spirit isn't that far from a bunch of regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('grows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('leaves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "import nltk; nltk.download('wordnet') # Download resource for working with WordNet via NLTK\n",
    "snowballer_stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snowballer_stemmer.stem('running'))\n",
    "print(snowballer_stemmer.stem('leaves'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('leaves'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Use the Porter stemmer to stem each word in the tweet dataset after having removed stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging\n",
    "\n",
    "POS tagging means assigning each token a part-of-speech (e.g. noun, verb, adjective, etc.). Again, there are many different [alternatives](https://github.com/nltk/nltk/tree/develop/nltk/tag), but NLTK keeps its recommended POS tagger available through the function `pos_tag`. The tagger expects a list of tokens as input.When doing POS tagging, it is advisable **not** to remove stop words beforehand (although you are free to do it afterwards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "single_review = reviews[3]\n",
    "single_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(single_review)\n",
    "import nltk; nltk.download('averaged_perceptron_tagger')\n",
    "tagged_review = pos_tag(tokens)\n",
    "tagged_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Below I've read in the text of Austen's _Pride and Prejudice_ into a variable called `pride`. Preprocess using the following steps:\n",
    "\n",
    "- Strip whitespace\n",
    "- Replace all numbers with '0'\n",
    "- Tokenize\n",
    "- Tag each token with a POS tag\n",
    "\n",
    "Make sure you know:\n",
    "- What type is the result?\n",
    "- What type is each element of the result?\n",
    "- What type are the elements of the elements of the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'pride-and-prejudice.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    raw = f.read()\n",
    "pride = raw[679:684814]\n",
    "pride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things we didn't cover\n",
    "(see `solutions/preprocessing_extra.ipynb` and [this repo](https://github.com/geoffbacon/nlp-with-nltk-spacy/blob/master/03-NLTK.ipynb) for more on these)\n",
    "\n",
    "- Reading in JSON, HTML, and XML files \n",
    "- Removing infrequent words\n",
    "- Named entity recognition\n",
    "- Syntactic parsing\n",
    "- Information extraction\n",
    "- Removing markup from HTML\n",
    "- Extracting numerical features\n",
    "- DTM/TF-IDF\n",
    "- SpaCy"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
