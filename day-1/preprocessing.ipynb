{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Like other data types, text data never comes clean. Moreover, most of our downstream methods only accept data structured in a particular way. Because of this, before we do any computational text analysis techniques, we will always need to perform some level of preprocessing on our text data. \n",
    "\n",
    "Different applications require different approaches to preprocessing. There is no \"one size fits all\" approach, and even if you use a program or module with built-in preprocessing steps, you want to know what it's doing (and not doing) so you can justify and/or change it. In some cases, it's better to do minimal preprocessing, to retain the valuable meanings stored in sentence structure and the like. In other cases, we want only a \"bag of words\", so more preprocessing is better. There are as many choices in preprocessing as there are in analysis (lots and lots!), so it's essential to understand the building blocks. \n",
    "\n",
    "For instance, to do corpus linguistics (e.g., syntactic parsing) or neural network models (e.g., word embeddings), we usually want to keep sentence structure and different word forms, so we would just tokenize and lower-case words, segment sentences, and normalize text (remove URLs, etc.). In contrast, for simpler techniques like finding the most distinctive or frequent words--as well as for topic models--we would typically do all that plus remove stopwords and punctuation and stem or lemmatize words. \n",
    "\n",
    "To help you understand these choice points so you can assemble your own domain-appropriate preprocessing recipes, in this notebook we will cover the basic ingredients.\n",
    "\n",
    "## Learning Goals\n",
    "* Get an introduction to the Python package NLTK, what functionality it offers, and learn some important NLP terms\n",
    "* Learn about the major choice points in preprocessing--and why you might or might not do them\n",
    "* Learn how to do a variety of forms of counting using NLTK, and think about why these might help researchers analyze text\n",
    "* Practice the basic ingredients in text preprocessing\n",
    "\n",
    "## Outline\n",
    "(TO DO: Add hyperlinks for each section)\n",
    "\n",
    "* [Reading in .txt and .csv files](#read)\n",
    "* [Tokenizing words](#tokenize)\n",
    "* [Sentence segmentation](#sentence)\n",
    "* [Removing punctuation](#punctuation)\n",
    "* [Stripping whitespace](#whitespace)\n",
    "    * Start and end of string\n",
    "    * Between words\n",
    "* [Text normalization](#normalize)\n",
    "    * Case folding\n",
    "    * Removing URLs, digits, and hashtags\n",
    "* [Counting word frequencies](#freqs)\n",
    "* [Removing stop words](#stopwords)\n",
    "* [Stemming/Lemmatizing](#stem)\n",
    "* [Parts-of-speech (POS) tagging](#POS)\n",
    "* [Document-Term Matrix (DTM)](#DTM)\n",
    "\n",
    "## Background\n",
    "\n",
    "We will do some review, but this notebook assumes you have basic familiarity with Python. If you need a beginner's introduction to coding in Python, please walk through the intro to Python notebook at `solutions/intro-to-python.ipynb` and/or [this one](https://github.com/lknelson/text-analysis-course/blob/master/scripts/01.25.02_PythonBasics.ipynb) *before* the workshop. \n",
    "\n",
    "We will also use some regular expressions, which are character sequences defining a search pattern. Usually this pattern is then used by string searching algorithms for \"find\" or \"find and replace\" operations. Don't worry if you haven't seen these before--we will keep it simple. If you want to get more out of this session, go through [this notebook on regular expressions](https://github.com/lknelson/text-analysis-course/blob/master/scripts/03.20.01_RegularExpressions.ipynb).\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "* *stop words*: \n",
    "    * The most common words in a language.\n",
    "* *tokenization*:\n",
    "    * Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. \n",
    "* *token*:\n",
    "    *  A token is an instance of a sequence of characters in some particular document that are grouped together as a useful semantic unit for processing.\n",
    "* *type*:\n",
    "    * A type is the class of all tokens containing the same character sequence.\n",
    "\n",
    "## Further Resources\n",
    "\n",
    "This is a great starting point, but there is a lot more to learn about preprocessing. To acquaint yourself with a few more tools, check out `solutions/preprocessing_extra.ipynb`, the other resources listed in [the workshop repo](https://github.com/jhaber-zz/nlp-python-2020), and the [useful intro scripts prepared by my colleague Laura Nelson](https://github.com/lknelson/text-analysis-course). To gain more practice and tools, come back for tomorrow's session!\n",
    "\n",
    "[Denny, Matthew J., and Arthur Spirling. 2018. “Text Preprocessing for Unsupervised Learning: Why It Matters, When It Misleads, and What to Do about It.” Political Analysis 26(2):168–189.\n",
    "](https://www.cambridge.org/core/journals/political-analysis/article/text-preprocessing-for-unsupervised-learning-why-it-matters-when-it-misleads-and-what-to-do-about-it/AA7D4DE0AA6AB208502515AE3EC6989E)\n",
    "<br>Documents importance of preprocessing choices to modeling results.\n",
    "\n",
    "**__________________________________**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in files<a id='read'></a>\n",
    "\n",
    "The first step is to read in the files containing the text data. The most common file types for text data are: `.txt`, `.csv`, `.json`, `.html` and `.xml`.\n",
    "\n",
    "### Reading in `.txt` files\n",
    "\n",
    "Python has built-in support for reading in `.txt` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "DATA_DIR = 'data'\n",
    "fname = 'pride-and-prejudice.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    raw = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review of Python string methods\n",
    "\n",
    "- What type of object is `raw`?\n",
    "- How many characters are in `raw`?\n",
    "- Get the first 1000 characters of `raw`.\n",
    "- Join together the first 200 and the last 200 characters of `raw`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in `.csv`\n",
    "\n",
    "Python has a built-in module called `csv` for reading in csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "fname = 'trump-tweets.csv'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "tweets = []\n",
    "\n",
    "# standard approach to opening files (no special encoding situations): # with open(fname) as f:\n",
    "import codecs\n",
    "with codecs.open(fname, \"r\", encoding='utf-8', errors='ignore') as f: # for special encoding issues  \n",
    "    reader = csv.reader(f)\n",
    "    tweets = list(reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review of Python list methods\n",
    "\n",
    "- What data type is `tweets`?\n",
    "- How many entries are in `tweets`?\n",
    "- Which entry is the header row?\n",
    "- Get the first 10 entries.\n",
    "- Join together the 5th and 10th elements of `tweets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in `.csv` with `pandas`\n",
    "\n",
    "`pandas` is a third-party library that makes working with tabular data much easier. This is the recommended way to read in a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "fname = 'trump-tweets.csv'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "tweets = pd.read_csv(fname) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review of `pandas`\n",
    "\n",
    "- What data type is `tweets`?\n",
    "- How many tweets are there?\n",
    "- What happened to the header row?\n",
    "- Get the first row of `tweets`.\n",
    "- Get the first 5 entries in the `Tweet_Text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in multiple files\n",
    "\n",
    "Often, our text data is split across multiple files in a folder. We want to read them all into a single variable. <br>`glob` is a handy package for this: it lists all files matching a pattern. We can use this to get all files in a folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "fnames = os.path.join(DATA_DIR, 'austen', '*.txt')\n",
    "fnames = glob.glob(fnames)\n",
    "austen = ''\n",
    "for fname in fnames:\n",
    "    with codecs.open(fname, \"r\", encoding='utf-8-sig', errors='ignore') as f:\n",
    "        text = f.read()\n",
    "        austen += text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review of working with files\n",
    "\n",
    "- What does `os.path.join()` do in this case?\n",
    "- What type is `fnames` after it is first assigned a value?\n",
    "- What type is `fnames` after it is assigned a second value?\n",
    "- How many files are in `fnames`?\n",
    "- What type is `austen`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Read in all the `.csv` files in the folder `amazon`. Extract out only the `text` column from THE FIRST TWO files and store them all in a list called `reviews`. \n",
    "\n",
    "**Hint 1:** Not all of these files heave a header row to indicate column names. But for your reference, the columns are in this order: <br>\n",
    "```Id, ProductId, UserId, ProfileName, HelpfulnessNumerator, HelpfulnessDenominator, Score, Time, Summary, Text```\n",
    "\n",
    "**Hint 2:** You can deal with `.csv` files without header rows by calling the argument `header=None` when loading into a pandas DataFrame. This lets pandas know not to mistake the first row of data for column names. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization<a id='tokenize'></a>\n",
    "\n",
    "Once we've read in the data, our next step is often to split it into words. This step is referred to as \"tokenization\", because each word occurrence is referred to as a \"token\". Each distinct word used is called a word \"type\". So the word type \"the\" may correspond to multiple tokens of \"the\" in a text. For example, the sentence \"the dog ate the cat\" has five tokens (\"the\", \"dog\", \"ate\", \"the\", \"cat\") but only *four* types (\"the\", \"dog\", \"ate\", \"cat\").\n",
    "\n",
    "There are many, many approaches to tokenization. The simplest approach to splitting words is by whitespace--the spacing between words. Let's see how to do that and compare it to a few other basic and accessible methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing by whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "fname = 'example1.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text.split()[:20] # tokenize by whitespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What problems do you notice with tokenizing by whitespace? If we're just looking for a \"bag of words\", what other steps should we take?\n",
    "- What type is `text`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing with regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "word_pattern = r'\\w+' # \\w means \"one or more words\": https://www.debuggex.com/cheatsheet/regex/python\n",
    "tokens = re.findall(word_pattern, text)\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What type is `tokens`?\n",
    "- What type is each element of `tokens`?\n",
    "- What does tokenizing with regular expressions do that tokenizing with whitespace does not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing with `nltk`\n",
    "\n",
    "[Just a bunch of regular expressions under the hood](https://github.com/nltk/nltk/blob/develop/nltk/tokenize/treebank.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk; nltk.download('punkt')\n",
    "tokens = word_tokenize(text)\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How does tokenization with NLTK compare to the previous approaches?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "A while ago you read in a bunch of Jane Austen books into a variable called `austen`. Tokenize that using a method of your choice. Find all the unique words types (you might want the `set` function). Sort the resulting set object to create a vocabulary (you might want to use the `sorted` function). \n",
    "\n",
    "**Extra credit:** Use what you know about types and tokens to measure lexical uniqueness--that is, how often Jane Austen reuses words. <br>*Hint:* Start by counting the number of types and the number of tokens in `austen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence segmentation<a id='sentence'></a>\n",
    "\n",
    "Sentence segmentation involves identifying the boundaries of sentences.\n",
    "\n",
    "### Sentence segmentation by splitting on periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.split('.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What does this simple approach do well? What does it miss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation with regular expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regular expressions allow us to split strings based on a number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_boundary_pattern = r'[.?!]' # This pattern identifies at least one punctuation mark concluding a sentence\n",
    "re.split(sent_boundary_pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence segmentation with `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge - SOLUTION\n",
    "\n",
    "The file `example2.txt` has more punctuation problems. Read it in (using same pattern as above) and see what the problems are. Try the different approaches above and see what works best. If you're already familiar with regular expressions, modify that code to work for as many cases as you can. What does this suggest about tokenizing text that includes punctuation?\n",
    "<br> (For help with regular expressions, see this [cheat sheet](https://www.debuggex.com/cheatsheet/regex/python))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing punctuation<a id='punctuation'></a>\n",
    "\n",
    "In some situations, it's best to keep only the alphanumeric characters (i.e. the letters and numbers) and ditch the punctuation. Here's how we can do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What type is `punctuation`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punct = ''.join([char for char in text if char not in punctuation])\n",
    "no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strip whitespace<a id='whitespace'></a>\n",
    "\n",
    "This is an extremely common step. It's simple to perform and nicely pre-packaged in Python. It's particularly common for user-generated text (think survey forms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = ' Hello! '\n",
    "string.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'example3.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()\n",
    "print(text) # This shows the whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and end of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stripped_text = text.strip() # This just strips whitespace from front and end--what about in between words?\n",
    "print(stripped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Between words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To strip whitespaces both at extremes and inside a text, regular expressions are a great tool:\n",
    "whitespace_pattern = r'\\s+'\n",
    "clean_text = re.sub(whitespace_pattern, ' ', text)\n",
    "clean_text.strip() # Strip whitespaces at start & end (again)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text normalization<a id='normalize'></a>\n",
    "\n",
    "Text normalization means making our text fit some standard patterns. Lots of steps come under this wide umbrella, but the most common are:\n",
    "\n",
    "- case folding\n",
    "- removing URLs, digits, hashtags\n",
    "- removing infequent words (not done here)\n",
    "\n",
    "### Case folding\n",
    "\n",
    "Case folding means dealing with upper and lower cases characters. This is usually done by making all characters lower-cased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'example4.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "The `lower` method we used above is a string method, that is, it works on strings. But say you've already tokenized a test, and you want to lowercase every word in the resulting list? Take the list of tokens below and make each one lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs, digits, and hashtags\n",
    "\n",
    "We rarely care about the exact URL used in a tweet, or the exact number of tweets. We could remove them completely, but it's often informative to know that there is a URL or a digit in the text using a placeholder (to avoid clouding our measures with the actual content). So we want to replace individual URLs and digits with a symbol that preserves the fact that a URL was there. It's standard to just use the strings \"URL\" and \"DIGIT\".\n",
    "\n",
    "How do we do this? Once again, regular expressions save the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get just the text for the tweets we read in earlier:\n",
    "tweet_text = tweets[\"Tweet_Text\"] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "single_tweet = tweet_text.iloc[0]\n",
    "single_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_SIGN = ' URL '\n",
    "re.sub(url_pattern, URL_SIGN, single_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we replaced the URL in a single tweet. Now we will replace all the URLs for each tweet in `tweet_text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "URL_SIGN = ' URL '\n",
    "list_of_url_less_tweets = []\n",
    "\n",
    "# Use a for loop\n",
    "for tweet in tweet_text:\n",
    "    url_less_tweet = re.sub(url_pattern, URL_SIGN, tweet)\n",
    "    list_of_url_less_tweets.append(url_less_tweet)\n",
    "list_of_url_less_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Alternative using list comprehension\n",
    "list_of_url_less_tweets = [re.sub(url_pattern, URL_SIGN, tweet) for tweet in tweet_text]\n",
    "list_of_url_less_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's remove hashtags and digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\n",
    "HASHTAG_SIGN = ' HASHTAG '\n",
    "digit_pattern = '\\d+'\n",
    "DIGIT_SIGN = ' DIGIT '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_hashtags = [re.sub(hashtag_pattern, HASHTAG_SIGN, tweet) for tweet in tweet_text]\n",
    "no_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_digit = [re.sub(digit_pattern, DIGIT_SIGN, tweet) for tweet in tweet_text]\n",
    "no_digit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting word frequencies<a id='freqs'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can count the frequency of each word type with the built-in `Counter` in Python. This basically just takes the set of word types (we calculated this above as `vocabulary`) and makes a special Python dictionary with each value being the number of times it appears in the list. We can ask that dictionary for the most common words, or for the frequency of individual word types. \n",
    "\n",
    "First, clean and normalize the text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = ' '.join(tweets)\n",
    "clean = re.sub(url_pattern, URL_SIGN, all_tweets)\n",
    "clean = re.sub(hashtag_pattern, HASHTAG_SIGN, clean)\n",
    "clean = re.sub(digit_pattern, DIGIT_SIGN, clean)\n",
    "tokens = word_tokenize(clean)\n",
    "tokens = [token for token in tokens if token not in punctuation]\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "freq = Counter(tokens)\n",
    "freq.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common way to count frequent words is with a built-in NLTK module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the nltk function FreqDist to count the number of times each token occurs.\n",
    "from nltk import FreqDist\n",
    "word_frequency = FreqDist(tokens)\n",
    "\n",
    "#this creates an NLTK object\n",
    "print(word_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(word_frequency.most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "I've read in some Amazon reviews from earlier into a list called `reviews`. Each element of the list is a string, representing the text of a single review. Try to:\n",
    "- Tokenize each review\n",
    "- Strip all whitespace\n",
    "- Make all characters lower case\n",
    "- Replace any URLs and digits\n",
    "\n",
    "Then find the most common 50 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = os.path.join(DATA_DIR, 'amazon', '*.csv')\n",
    "fnames = glob.glob(fnames)\n",
    "reviews = []\n",
    "column_names = ['id', 'product_id', 'user_id', 'profile_name', 'helpfulness_num', 'helpfulness_denom',\n",
    "               'score', 'time', 'summary', 'text']\n",
    "for fname in fnames[:2]:\n",
    "    df = pd.read_csv(fname, names=column_names)\n",
    "    text = list(df['text'])\n",
    "    reviews.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words<a id='stopwords'></a>\n",
    "\n",
    "You might have noticed that the most common words above aren't terribly exciting. They're words like \"am\", \"i\", \"the\" and \"a\": stop words. These are rarely useful to us in computational text analysis, so it's very common to remove them completely.\n",
    "\n",
    "- What other stop words do you think there are?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk; nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Use the list `stop` of English stopwords to remove stopwords from our tokenized review above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming/lemmatization<a id='stem'></a>\n",
    "\n",
    "Stemming and lemmatization both refer to remove morphological affixes on words. For example, if we stem the word \"grows\", we get \"grow\". If we stem the word \"running\", we get \"run\". We do this because often we care more about the core content of the word (i.e. that it has something to do with growth or running, rather than the fact that it's a third person present tense verb, or progressive participle).\n",
    "\n",
    "NLTK provides many algorithms for stemming. For English, a great baseline is the [Porter](https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py) algorithm, which is in spirit isn't that far from a bunch of regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('grows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('running')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('leaves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "import nltk; nltk.download('wordnet') # Download resource for working with WordNet via NLTK\n",
    "snowballer_stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snowballer_stemmer.stem('running'))\n",
    "print(snowballer_stemmer.stem('leaves'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize('leaves'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Use the Porter stemmer to stem each word in the tweet dataset after having removed stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS tagging<a id='POS'></a>\n",
    "\n",
    "POS tagging means assigning each token a part-of-speech (e.g. noun, verb, adjective, etc.). Again, there are many different [alternatives](https://github.com/nltk/nltk/tree/develop/nltk/tag), but NLTK keeps its recommended POS tagger available through the function `pos_tag`. The tagger expects a list of tokens as input.When doing POS tagging, it is advisable **not** to remove stop words beforehand (although you are free to do it afterwards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "single_review = reviews[3]\n",
    "single_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(single_review)\n",
    "import nltk; nltk.download('averaged_perceptron_tagger')\n",
    "tagged_review = pos_tag(tokens)\n",
    "tagged_review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge \n",
    "\n",
    "Below I've read in the text of Austen's _Pride and Prejudice_ into a variable called `pride`. Preprocess using the following steps:\n",
    "\n",
    "- Strip whitespace\n",
    "- Replace all numbers with '0'\n",
    "- Tokenize\n",
    "- Tag each token with a POS tag\n",
    "\n",
    "Make sure you know:\n",
    "- What type is the result?\n",
    "- What type is each element of the result?\n",
    "- What type are the elements of the elements of the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'pride-and-prejudice.txt'\n",
    "fname = os.path.join(DATA_DIR, fname)\n",
    "with open(fname, encoding='utf-8') as f:\n",
    "    raw = f.read()\n",
    "pride = raw[679:684814]\n",
    "pride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your solution here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document-Term Matrix<a id='DTM'></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Things we didn't cover\n",
    "(see `solutions/preprocessing_extra.ipynb` and [this repo](https://github.com/geoffbacon/nlp-with-nltk-spacy/blob/master/03-NLTK.ipynb) for more on these)\n",
    "\n",
    "- Reading in JSON, HTML, and XML files \n",
    "- Removing infrequent words\n",
    "- Named entity recognition\n",
    "- Scikit-learn preprocessing pipelines\n",
    "- Syntactic parsing\n",
    "- Information extraction\n",
    "- Removing markup from HTML\n",
    "- Extracting numerical features\n",
    "- Term Frequency Inverse Document Frequency (TF-IDF)\n",
    "- SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
