{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Methods\n",
    "\n",
    "How do we know what texts are about? One of the simplest ways is called dictionary methods, which counts words of interest in text. This is useful if we know what categories we're interested in and can specify words related to them. It's also one of the most long-standing, and ubiquitous, methods in automated text analysis, so it's important to both understand the method and be able to implement it.\n",
    "\n",
    "Dictionary methods are used for many purposes. A few possibilities:\n",
    "* classify text into themes\n",
    "* measure the *tone* of text\n",
    "* measure sentiment\n",
    "* measure psychological processes\n",
    "\n",
    "Dictionary methods are based on the assumption that themes or categories consist of a group of words, and texts that cover that theme will have a higher percentage of that group of words compared to other texts. Dictionary-based analysis is appropriate when the categories and the textual features (words and/or phrases associated with each category) are known and fixed--based on expert knowledge, crowd-sourcing, etc. \n",
    "\n",
    "We will use dictionary methods to do sentiment analysis, a popular text analysis task, on a corpus of Music Reviews (collected from MetaCritic.com), using a standard sentiment analysis dictionary. We will also use a weighted dictionary to detect concreteness in novels. Finally, we will explore using word weighting to get distinctive words.\n",
    "\n",
    "## Outline\n",
    "* [Part 0: Basic dictionary method](#basic)\n",
    "    0. [Introduction to dictionary methods](#intro)\n",
    "        * Standard dictionaries\n",
    "        * Custom dictionaries\n",
    "    1. [Preprocessing](#preprocess)\n",
    "    2. [Creating dictionary counts](#counts)\n",
    "    3. [Sentiment analysis using Scikit-learn](#sentiment)\n",
    "<br><br>    \n",
    "* [Part 1: Weighted dictionary](#weighting)\n",
    "    0. [Read concreteness score dictionary](#read)\n",
    "    1. [Merging a DTM with a weighted dictionary](#merge)\n",
    "    2. [Weight term frequencies by their concreteness score](#concweight)\n",
    "    3. [Calculating an average concreteness score for each text](#average)\n",
    "    4. [Assess the difference](#assess)\n",
    "<br><br>\n",
    "* [Bonus: Weighting words with TF-IDF](#tfidf)\n",
    "    * Identifying distinctive words\n",
    "<br><br>\n",
    "* [Further resources](#resources)\n",
    "\n",
    "## Learning Goals\n",
    "* Understand the intuition behind dictionary methods\n",
    "* Learn how to implement in via Python Pandas and NLTK\n",
    "* Get more comfortable combining Python packages together\n",
    "* Implement a rudimentary sentiment analysis tool and test it on sample data\n",
    "* Practice applying a weighted dictionary\n",
    "\n",
    "## Vocabulary\n",
    "\n",
    "* *dictionary method*:\n",
    "    * text analysis method that utilizes the frequency of key words, grouped into themes, to determine the prevelance of that theme throughout a corpus.\n",
    "* *standard dictionary*:\n",
    "    * otherwise known as general dictionaries, a dictionary created by experts meant to measure general phenomenon.\n",
    "* *custom dictionary*:\n",
    "    * dictionaries tailored to a specific domain or question. Usually created by the researcher based on the research question.\n",
    "* *sentiment analysis*:\n",
    "    * the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc., is positive, negative, or neutral.\n",
    "* *Term Frequency-Inverse Document Frequency (TF-IDF) Scores*: \n",
    "    *  a numerical statistic intended to reflect how important a word is to a document in a collection or corpus. As with DTM, rows correspond to documents in the collection and columns correspond to terms. Making TF-IDF scores is a common preprocessing step: it takes in tokenized texts and makes them ready for downstream tasks (e.g., topic models, supervised machine learning models)\n",
    "\n",
    "**__________________________________**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Basic dictionary methods<a id='basic'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Introduction to dictionary methods<a id='intro'></a>\n",
    "\n",
    "The basic idea behind dictionary methods is that language reflects social position and culture: the cognitive categories through which individuals attend to the world are embedded in the words they use.  Words that are used frequently are cognitively central and reflect what is most on the speaker’s (or writer’s) mind.  Words that are used infrequently or not at all are at the speaker’s cognitive periphery, perhaps even representing uncomfortable or alien concepts.\n",
    "\n",
    "There are two forms of dictionaries: standard or general dictionaries, and custom dictionaries.\n",
    "\n",
    "### Standard dictionaries\n",
    "\n",
    "There are a number of standard dictionaries that have been created by field experts. The benefit of standarized dictionaries is that they're developed by experts and have been throughoughly validated. Others have likely published using these dictionaries, so reviewers are more likely to accept them as valid. Because of this, they are good options if they fit your research question.\n",
    "\n",
    "Here are a few:\n",
    "\n",
    "* [DICTION](http://www.dictionsoftware.com/): a computer-aided text analysis program for determining the tone of a text. It was created by and for organization scholars and political scientists.\n",
    "    * Main five categories: Certainty, Activity, Optimism, Realism, Commonality\n",
    "    * 35 sub-categories\n",
    "    * Allows you to create your own dictionary\n",
    "    * Proprietary software\n",
    "* [Linguistic Inquiry and Word Count (LIWC)](http://liwc.wpengine.com/): Created by psychologists, it's meant to capture psychological processes around feelings, personality, and motivations. It's also proprietary.\n",
    "* [Multi-Perspective Question Answering (MPQA)](http://mpqa.cs.pitt.edu/): The free version of LIWC. We will use this dictionary today.\n",
    "* [Valence Aware Dictionary and sEntiment Reasoner (VADER)](https://github.com/cjhutto/vaderSentiment): a popular and free rule-based sentiment analysis tool tuned to capture sentiment in social media data.\n",
    "* [Harvard General Inquirer](http://www.wjh.harvard.edu/~inquirer/spreadsheet_guide.htm). Multiple categories, including abstract and concrete words. It's free and available online.\n",
    "\n",
    "However, dictionaries are context-specific, and applying a dictionary outside its original target domain can lead to biased or  unreliable results. A classic example was applying the Harvard General Inquirer to classify negative tone in corporate earnings reports--which [leads to serious problems](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-6261.2010.01625.x?casa_token=Vl0Os0QtDIQAAAAA%3AaxNRNAj4ajoocjLv_DCJMsS8GEA_jAqb_Z26h5I8g1yW0muShiPs_lglFbpHR-3AU1etsuyjhdN_ahZy). For example, the term \"vice\" characterizes an executive rather than immorality, while \"tire\" is not negative in the context of automobile industry reports. And some negative words aren't captured at all: the terms \"litigation\" and \"unanticipated\" are missing from the Harvard list. Enough of these problems means the results of our analyses will be wrong. So sometimes it's worth the effort to create a custom dictionary.\n",
    "\n",
    "### Custom dictionaries\n",
    "\n",
    "Many research questions or data are domain specific and will thus require you to create your own dictionary based on your own knowledge of the domain and research question. Creating your own dictionary requires a lot of thought and must be validated. These dictionaries are typically created in an iterative fashion; they are first formed from domain knowledge, and as the validation process gets going they may be modified again and again. See references & links (at bottom of this notebook) to Enns et al. (2015) or Haber (2020) for examples of how to construct a domain-specific dictionary. \n",
    "\n",
    "Today we will use the free and standard sentiment dictionary from MPQA to measure positive and negative sentiment in the music reviews.\n",
    "\n",
    "We are about to apply an NLP model. Do you remember what comes first?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Preprocessing<a id='preprocess'></a>\n",
    "\n",
    "### Review of reading in multiple files\n",
    "\n",
    "Often, our text data is split across multiple files in a folder. We want to read them all into a single variable. <br>`glob` is a handy package for this: it lists all files matching a pattern. We can use this to get all files in a folder. \n",
    "\n",
    "As a demonstration, let's see how to read in all the `.csv` files in the folder `amazon`. We'll extract out only the `text` column from THE FIRST TWO files and store them all in a list called `reviews`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "\n",
    "fnames = os.path.join('../day-1/data/', 'amazon', '*.csv') # get file paths\n",
    "fnames = glob.glob(fnames)\n",
    "\n",
    "# Most elegant or 'pythonic' way to do this: a for loop\n",
    "reviews = [] # ***VERY IMPORTANT:*** initialize empty list\n",
    "column_names = [\"Id\", \"ProductId\", \"UserId\", \"ProfileName\", \"HelpfulnessNumerator\", \"HelpfulnessDenominator\", \"Score\", \"Time\", \"Summary\", \"Text\"] # define column names\n",
    "\n",
    "# Loop over fnames, for each one create DataFrame and extract txt \n",
    "for fname in fnames[:2]:\n",
    "    df = pd.read_csv(fname, names=column_names) # read each .csv into a DataFrame, coerce column names\n",
    "    text = list(df['Text']) # extract `text` column\n",
    "    reviews.extend(text) # add to `reviews` list\n",
    "\n",
    "reviews[:3] # preview results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review of removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuations = list(string.punctuation)\n",
    "\n",
    "fname = 'example2.txt'\n",
    "fname = os.path.join('../day-1/data/', fname)\n",
    "with open(fname) as f:\n",
    "    text = f.read()\n",
    "    \n",
    "# Note the list comprehension--this is the most common way to remove punctuation:\n",
    "no_punct = ''.join([char for char in text if char not in punctuations])\n",
    "no_punct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New data\n",
    "Let's read in our Music Reviews corpus as a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary packages\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#read the Music Reviews corpus into a Pandas dataframe\n",
    "df = pd.read_csv(\"../day-2/data/BDHSI2016_music_reviews.csv\", encoding='utf-8', sep = '\\t')\n",
    "\n",
    "#view the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at breakdown of genres:\n",
    "df['genre'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to create a new column in our dataset that contains tokenized words with all the pre-processing steps. First, let's remove digits, then you can practice by doing the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove digits from `body` column:\n",
    "df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Let's review preprocessing using the `df` we just created. This is a little different from yesterday's practice using strings and lists, but the essentials are the same. To see the key new things you'll likely want to use, refer to the example of removing digits from the previous cell--especially note the list comprehension and this useful strategy: \n",
    "`df['column'].apply(lambda x: function(x))`. \n",
    "\n",
    "To preprocess `df`, take these steps:\n",
    "* Create a new column in `df` called `body_tokens` that contains a lower cased version of `df['body']`. \n",
    "* Tokenize the `body_tokens` column of `df` using one of the methods we worked with yesterday. \n",
    "* Remove punctuation from `body_tokens`. \n",
    "* Create a new column that contains the length of the token list in each row. We will use this later to normalize the dictionary counts. \n",
    "* Reflect: What other pre-processing steps might we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Creating dictionary counts<a id='counts'></a>\n",
    "\n",
    "I created two text files, one is a list of positive words from the MPQA dictionary, the other is a list of negative words. One word per line. Our goal here is to count the number of positive and negative words in each row of our dataframe, and add two columns to our dataset with the count of positive and negative words.\n",
    "\n",
    "First, read in the positive and negative words and create list variables for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_str = open(\"../day-2/data/positive_words.txt\", encoding='utf-8').read()\n",
    "neg_str = open(\"../day-2/data/negative_words.txt\", encoding='utf-8').read()\n",
    "\n",
    "#view part of the pos_sent variable, to see how it's formatted.\n",
    "pos_str[:101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember the split function? We'll split on the newline character (\\n) to create a list\n",
    "positive_words=pos_str.split('\\n')\n",
    "negative_words=neg_str.split('\\n')\n",
    "\n",
    "#view every 100th element in the lists\n",
    "print(positive_words[::100])\n",
    "print(negative_words[::100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count number of words in each list\n",
    "print('This many words in positive MPQA dictionary:', str(len(positive_words)))\n",
    "print('This many words in negative MPQA dictionary:', str(len(negative_words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! You know what to do now.\n",
    "\n",
    "### Challenge\n",
    "1. Create a column with the number of positive words, and another with the proportion of positive words\n",
    "2. Create a column with the number of negative words, and another with the proportion of negative words\n",
    "3. Print the average proportion of negative and positive words by genre\n",
    "4. Compare this to the average score by genre\n",
    "\n",
    "*Note:* You won't be able to do this challenge (or anything else in this section) if you didn't complete the first challenge above to preprocess `df['body']` into `df['body_tokens']`. If you skipped that part or got stuck, copy and run the solution from `solutions/dictionary-methods-solutions.ipynb` before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's the dictionary method! You can do this with any dictionary you want, standard or you can create your own.\n",
    "\n",
    "## 0.3. Sentiment analysis using scikit-learn<a id='sentiment'></a>\n",
    "\n",
    "We can also do this using a Document-Term Matrix. We'll do this in pandas, to make it conceptually clear. As you get more comfortable with programming, you will probably want to get used to working with Compressed Sparse Format, which is much more computationally efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the function CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer()\n",
    "\n",
    "#create our document term matrix as a pandas dataframe\n",
    "dtm_df = pd.DataFrame(countvec.fit_transform(df.body).toarray(), columns=countvec.get_feature_names(), index = df.index)\n",
    "dtm_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can keep only those *columns* that occur in our positive words list. To do this, we'll first save a list of the columns names as a variable, and then only keep the elements of the list that occur in our positive words list. We'll then create a new dataframe keeping only those select columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a columns variable that is a list of all column names\n",
    "columns = list(dtm_df)\n",
    "columns[::750] # view every 750th element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new variable that contains only column names that are in our postive words list\n",
    "pos_columns = [word for word in columns if word in positive_words]\n",
    "pos_columns[::100] # view every 100th element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dtm from our dtm_df that keeps only positive sentiment columns\n",
    "dtm_pos = dtm_df[pos_columns]\n",
    "dtm_pos.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the number of positive words for each document\n",
    "dtm_pos['pos_count'] = dtm_pos.sum(axis=1)\n",
    "dtm_pos['pos_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "1. Do the same for negative words.  \n",
    "2. Calculate the proportion of negative and positive words for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Weighting dictionaries<a id='weighting'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use a weighted dictionary to compare the relative average concreteness of the words used in Austen's *Pride and Prejudice* versus Alcott's *A Garland for Girls*. A weighted dictionary indicates not only whether a phrase is associated with a category, but *how strongly* it is associated with that category. In this approach, a dictionary is a list of weighted words.\n",
    "\n",
    "This could be done using a regular dictionary: a list of concrete and abstract words. Instead, we'll use a crowdsourced dictionary that provides an average \"concreteness score\" for a large number of English words.\n",
    "\n",
    "## 1.1 Read concreteness score dictionary<a id='read'></a>\n",
    "\n",
    "First we'll create a pandas dataframe from the concreteness score dictionary, saved on our hard drive in the form of a .csv file.\n",
    "\n",
    "This dictionary comes from work by [Marc Brysbaert, Amy Beth Warriner, and Victor Kuperman.](https://link.springer.com/article/10.3758/s13428-013-0403-5) In summary:\n",
    "\n",
    "    The authors obtained Concreteness ratings for 37,058 English words and 2,896 two-word expressions (such as zebra crossing and zoom in), by means of a norming study using Internet crowdsourcing for data collection. They had over 4,000 participants rate 5 words on a concreteness scale, from 1 (very abstract) to 5 (very concrete). They define concrete words as words you can experience through the senses, and abstract words as words that you cannot experience through the senses. They provide the average concreteness score and the standard deviation for each word.\n",
    "\n",
    "Let's read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_score = pd.read_csv('../day-2/data/Concreteness_ratings_Brysbaert_et_al.csv')\n",
    "con_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the most concrete and most abstract words by sorting on `Conc.M`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_score[['Word','Conc.M']].sort_values(by='Conc.M',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Merging a DTM with a weighted dictionary<a id='merge'></a>\n",
    "\n",
    "The goal is to merge this score with our Document Term Matrix, so we can calculate the average concreteness score for our texts.\n",
    "\n",
    "To do this, we'll first create the DTM from our two novels, transpose this matrix, and merge it with the dataframe created above. We'll merge on the column 'Word'.\n",
    "\n",
    "First, create the DTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [] # initialize list\n",
    "\n",
    "# open and read the novels, save them as variables\n",
    "austen_string = open('../day-2/data/Austen_PrideAndPrejudice.txt', encoding='utf-8').read()\n",
    "alcott_string = open('../day-2/data/Alcott_GarlandForGirls.txt', encoding='utf-8').read()\n",
    "\n",
    "# append each novel to the list\n",
    "text_list.append(austen_string)\n",
    "text_list.append(alcott_string)\n",
    "\n",
    "countvec = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "novels_df = pd.DataFrame(countvec.fit_transform(text_list).toarray(), columns=countvec.get_feature_names())\n",
    "novels_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll take a subset of the DTM, keeping only the intersection between the words in our corpus and the words in the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=list(novels_df)\n",
    "columns_con = [word for word in columns if word in list(con_score['Word'])]\n",
    "columns_con[::300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df_con = novels_df[columns_con]\n",
    "novels_df_con "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, transpose the matrix, rename the column, and merge with the dictionary dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = novels_df_con.transpose() # transpose\n",
    "\n",
    "df.rename(columns={0: 'Austen', 1: 'Alcott'}, inplace=True) # rename\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the index 'Word', and reset the index, so the words become a column in our dataframe and we get a new index.\n",
    "df.index.names = ['Word']\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with our dictionary dataframe, called 'con_score'\n",
    "df = df.merge(con_score, on = 'Word')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Weighting term frequencies by the concreteness score<a id='concweight'></a>\n",
    "\n",
    "Now we can weight the term frequency cells by the concreteness score, by multiplying the frequency count column by the concreteness score column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['austen_con_score'] = df['Austen'] * df['Conc.M']\n",
    "df['alcott_con_score'] = df['Alcott'] * df['Conc.M']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Calculate and print the average concreteness score for each text. Careful! Think through this before you implement it. You want the average score, normalized over all the words in the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Assessing the difference<a id='assess'></a>\n",
    "\n",
    "So there is a difference, but what does it mean? What is the magnitude of the difference?\n",
    "\n",
    "We can look at the difference between the two means as a percent difference based on the scale range. We can calculate this using simple math."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First find the difference between the means by substracting one from the other\n",
    "3.1534507874-2.78328905828"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the range of concreteness scores\n",
    "print(df['Conc.M'].min())\n",
    "print(df['Conc.M'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the scale range\n",
    "df['Conc.M'].max() - df['Conc.M'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the difference of means as a percent of this range\n",
    "(0.37/3.83)* 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "Print the most concrete and abstract terms in Austen and in Alcott. Don't worry about term frequencies; just look at the raw score of words present in each novel.<br>\n",
    "*Hint:* You can't simply sort on the column `austen_con_score` and so on. Why not? What are your next steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Weighting words with TF-IDF<a id='tfidf'></a>\n",
    "\n",
    "Next let's practice with Term Frequency-Inverse Document Frequency (TF-IDF) word scores. This isn't a dictionary method, but rather a weighted version of what we practiced yesterday: the Document-Term Matrix (DTM). Using TF-IDF will allow us to find distinctive words in our dataset.\n",
    "\n",
    "The idea behind word scores (like TF-IDF) in general is to weight words not just by their frequency (as in the weighted dictionaries we just practiced), but by their frequency in one document compared to their distribution across all documents. Words that are frequent, but are also used in every single document, will not be indicative of the content of that document. We want to instead identify frequent words that are unevenly distributed across the corpus.\n",
    "\n",
    "TF-IDF is one of the most popular ways to weight word scores (aside from counting raw word frequencies). By offsetting the frequency of a word (TF) by its document frequency (the number of documents in which it appears; IDF), we can filter out common or stop words like 'the', 'of', and 'and'.\n",
    "\n",
    "More precisely, the IDF is calculated as the inverse proportion of documents with a given word:\n",
    "\n",
    "`number_of_documents / number_documents_with_word`\n",
    "\n",
    "This together with the raw frequency of the word in a given document gives:\n",
    "\n",
    "`tfidf_word1 = word1_frequency_document1 * (number_of_documents / number_document_with_word1)`\n",
    "\n",
    "This will give greater scores to words in long documents. To keep word scores on a comparable scale, it's usually best to normalize the numerator: \n",
    "\n",
    "`tfidf_word1 = (word1_frequency_document1 / word_count_document1) * (number_of_documents / number_document_with_word1)`\n",
    "\n",
    "Scikit-learn is the standard method of implementing TF-IDF in Python, and the main class for this is [TfidfVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer). This notebook uses the Scikit-learn method, but here's a special challenge for those willing: Try calculating TF-IDF manually!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our Music Reviews corpus for this. Read into Pandas DataFrame:\n",
    "df = pd.read_csv(\"../day-2/data/BDHSI2016_music_reviews.csv\", encoding='utf-8', sep = '\\t')\n",
    "\n",
    "# To avoid muddying things, clean out numbers--but leave stopwords, etc. in place for now:\n",
    "df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the function TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvec = TfidfVectorizer()\n",
    "\n",
    "#create the dtm, but with cells weighted by the TF-IDF word score\n",
    "tfidf_df = pd.DataFrame(tfidfvec.fit_transform(df['body']).toarray(), columns=tfidfvec.get_feature_names())\n",
    "\n",
    "#view results\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the 20 words with highest tf-idf weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tfidf_df.max(axis=0).sort_values(ascending=False)[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok! We have successfully identified content words, without removing stop words and without part-of-speech tagging. What else do you notice about this list?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Distinctive Words\n",
    "\n",
    "What can we do with this? These scores are best used when you want to identify distinctive words for individual documents, or groups of documents, compared to other groups or the corpus as a whole. To illustrate this, let's compare three genres and identify the most distinctive words by genre.\n",
    "\n",
    "First we merge the genre of the document into our DTM weighted by TF-IDF scores, and then compare genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat dataset with document index and genre\n",
    "df_genre = df['genre'].to_frame()\n",
    "print(df_genre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge this into the tfidf_df (DTM weighted by TF-IDF)\n",
    "merged_df = df_genre.join(tfidf_df, how = 'right', lsuffix='_x')\n",
    "\n",
    "#view result\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare the words with the highest TF-IDF weight for each genre. The illustrating question: **what words distinguish reviews of Rap albums, Indie Rock albums, and Jazz albums?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull out the reviews for three genres, Rap, Alternative/Indie Rock, and Jazz\n",
    "dtm_rap = merged_df[merged_df['genre_x']=='Rap']\n",
    "dtm_indie = merged_df[merged_df['genre_x']=='Alternative/Indie Rock']\n",
    "dtm_jazz = merged_df[merged_df['genre_x']=='Jazz']\n",
    "\n",
    "#print the words with the highest TF-IDF scores for each genre\n",
    "print('Rap Words')\n",
    "print(dtm_rap.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print('Indie Words')\n",
    "print(dtm_indie.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print('Jazz Words')\n",
    "print(dtm_jazz.max(numeric_only=True).sort_values(ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There we go! A method of identifying content words, and distinctive words based on groups of texts. You notice there are some proper nouns in there. How might we remove those if we're not interested in them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Compare the distinctive words for two artists in the data.\n",
    "\n",
    "Note: the artists should have a number of reviews, so check your frequency counts to identify artists.\n",
    "\n",
    "*Hint:* Copy and paste the above code and modify it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Resources<a id='resources'></a>\n",
    "\n",
    "[Gonçalves, Pollyanna, Matheus Araújo, Fabrício Benevenuto, and Meeyoung Cha. 2013. \"Comparing and Combining Sentiment Analysis Methods\"](https://dl.acm.org/doi/abs/10.1145/2512938.2512951)\n",
    "<br>Compares eight popular sentiment analysis methods (including LIWC and others) and finds wide variation in their results.\n",
    "\n",
    "[Jockers, Matthew L. 2015. \"A Novel Method for Detecting Plot\"](http://www.matthewjockers.net/2014/06/05/a-novel-method-for-detecting-plot/)\n",
    "<br>Blog applying sentiment analysis to several classic novels. \n",
    "\n",
    "Examples of creating custom dictionaries:\n",
    "- [Haber, Jaren. 2020. “Sorting Schools: A Computational Analysis of Charter School Identities and Stratification”](https://doi.org/10.1177/0038040720953218) <br>\n",
    "- [Enns, Peter, Nathan Kelly, Jana Morgan, and Christopher Witko. 2015. \"Money and the Supply\n",
    "of Political Rhetoric: Understanding the Congressional (Non-)Response to Economic Inequality”](http://cdn.equitablegrowth.org/wp-content/uploads/2016/06/29155322/enns-kelly-morgan-witko-econinterests-policyagenda.pdf) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
