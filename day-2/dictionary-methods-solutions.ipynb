{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary Methods - Solutions to Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 0: Basic dictionary methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>release_date</th>\n",
       "      <th>critic</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't Panic</td>\n",
       "      <td>All Time Low</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>2012-10-09 00:00:00</td>\n",
       "      <td>Kerrang!</td>\n",
       "      <td>74.0</td>\n",
       "      <td>While For Baltimore proves they can still writ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fear and Saturday Night</td>\n",
       "      <td>Ryan Bingham</td>\n",
       "      <td>Country</td>\n",
       "      <td>2015-01-20 00:00:00</td>\n",
       "      <td>Uncut</td>\n",
       "      <td>70.0</td>\n",
       "      <td>There's nothing fake about the purgatorial nar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Way I'm Livin'</td>\n",
       "      <td>Lee Ann Womack</td>\n",
       "      <td>Country</td>\n",
       "      <td>2014-09-23 00:00:00</td>\n",
       "      <td>Q Magazine</td>\n",
       "      <td>84.0</td>\n",
       "      <td>All life's disastrous lows are here on a caree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doris</td>\n",
       "      <td>Earl Sweatshirt</td>\n",
       "      <td>Rap</td>\n",
       "      <td>2013-08-20 00:00:00</td>\n",
       "      <td>Pitchfork</td>\n",
       "      <td>82.0</td>\n",
       "      <td>With Doris, Odd Future’s Odysseus is finally b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giraffe</td>\n",
       "      <td>Echoboy</td>\n",
       "      <td>Rock</td>\n",
       "      <td>2003-02-25 00:00:00</td>\n",
       "      <td>AllMusic</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Though Giraffe is definitely Echoboy's most im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Outer South</td>\n",
       "      <td>Conor Oberst And The Mystic Valley Band</td>\n",
       "      <td>Indie</td>\n",
       "      <td>2009-05-05 00:00:00</td>\n",
       "      <td>Slant Magazine</td>\n",
       "      <td>67.0</td>\n",
       "      <td>The result is an album that's unfortunately ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>On An Island</td>\n",
       "      <td>David Gilmour</td>\n",
       "      <td>Rock</td>\n",
       "      <td>2006-03-07 00:00:00</td>\n",
       "      <td>E! Online</td>\n",
       "      <td>67.0</td>\n",
       "      <td>In the end, Island makes Dave sound like he's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Movement</td>\n",
       "      <td>Gossip</td>\n",
       "      <td>Indie</td>\n",
       "      <td>2003-05-06 00:00:00</td>\n",
       "      <td>Uncut</td>\n",
       "      <td>81.0</td>\n",
       "      <td>Beth Ditto's remarkable gospel holler and ferv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Locked Down</td>\n",
       "      <td>Dr. John</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>2012-04-03 00:00:00</td>\n",
       "      <td>PopMatters</td>\n",
       "      <td>86.0</td>\n",
       "      <td>Dr. John is Dr. John. He's a star, and is on f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>And Their Refinement Of The Decline</td>\n",
       "      <td>Stars Of The Lid</td>\n",
       "      <td>Rock</td>\n",
       "      <td>2007-04-07 00:00:00</td>\n",
       "      <td>PopMatters</td>\n",
       "      <td>87.0</td>\n",
       "      <td>Their work, especially that displayed on Refin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5001 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    album  \\\n",
       "0                             Don't Panic   \n",
       "1                 Fear and Saturday Night   \n",
       "2                      The Way I'm Livin'   \n",
       "3                                   Doris   \n",
       "4                                 Giraffe   \n",
       "...                                   ...   \n",
       "4996                          Outer South   \n",
       "4997                         On An Island   \n",
       "4998                             Movement   \n",
       "4999                          Locked Down   \n",
       "5000  And Their Refinement Of The Decline   \n",
       "\n",
       "                                       artist     genre         release_date  \\\n",
       "0                                All Time Low  Pop/Rock  2012-10-09 00:00:00   \n",
       "1                                Ryan Bingham   Country  2015-01-20 00:00:00   \n",
       "2                              Lee Ann Womack   Country  2014-09-23 00:00:00   \n",
       "3                             Earl Sweatshirt       Rap  2013-08-20 00:00:00   \n",
       "4                                     Echoboy      Rock  2003-02-25 00:00:00   \n",
       "...                                       ...       ...                  ...   \n",
       "4996  Conor Oberst And The Mystic Valley Band     Indie  2009-05-05 00:00:00   \n",
       "4997                            David Gilmour      Rock  2006-03-07 00:00:00   \n",
       "4998                                   Gossip     Indie  2003-05-06 00:00:00   \n",
       "4999                                 Dr. John  Pop/Rock  2012-04-03 00:00:00   \n",
       "5000                         Stars Of The Lid      Rock  2007-04-07 00:00:00   \n",
       "\n",
       "              critic  score                                               body  \n",
       "0           Kerrang!   74.0  While For Baltimore proves they can still writ...  \n",
       "1              Uncut   70.0  There's nothing fake about the purgatorial nar...  \n",
       "2         Q Magazine   84.0  All life's disastrous lows are here on a caree...  \n",
       "3          Pitchfork   82.0  With Doris, Odd Future’s Odysseus is finally b...  \n",
       "4           AllMusic   71.0  Though Giraffe is definitely Echoboy's most im...  \n",
       "...              ...    ...                                                ...  \n",
       "4996  Slant Magazine   67.0  The result is an album that's unfortunately ba...  \n",
       "4997       E! Online   67.0  In the end, Island makes Dave sound like he's ...  \n",
       "4998           Uncut   81.0  Beth Ditto's remarkable gospel holler and ferv...  \n",
       "4999      PopMatters   86.0  Dr. John is Dr. John. He's a star, and is on f...  \n",
       "5000      PopMatters   87.0  Their work, especially that displayed on Refin...  \n",
       "\n",
       "[5001 rows x 7 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the necessary packages\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "#read the Music Reviews corpus into a Pandas dataframe\n",
    "df = pd.read_csv(\"../day-2/data/BDHSI2016_music_reviews.csv\", encoding='utf-8', sep = '\\t')\n",
    "\n",
    "#view the dataframe\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove digits from `body` column:\n",
    "df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge - SOLUTION\n",
    "Let's review preprocessing using the `df` we just created. This is a little different from yesterday's practice using strings and lists, but the essentials are the same. To see the key new things you'll likely want to use, refer to the example of removing digits from the previous cell--especially note the list comprehension and this useful strategy: \n",
    "`df['column'].apply(lambda x: function(x))`. \n",
    "\n",
    "To preprocess `df`, take these steps:\n",
    "* Create a new column in `df` called `body_tokens` that contains a lower cased version of `df['body']`. \n",
    "* Tokenize the `body_tokens` column of `df` using one of the methods we worked with yesterday. \n",
    "* Remove punctuation from `body_tokens`. \n",
    "* Create a new column that contains the length of the token list in each row. We will use this later to normalize the dictionary counts. \n",
    "* Reflect: What other pre-processing steps might we use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       while for baltimore proves they can still writ...\n",
       "1       there's nothing fake about the purgatorial nar...\n",
       "2       all life's disastrous lows are here on a caree...\n",
       "3       with doris, odd future’s odysseus is finally b...\n",
       "4       though giraffe is definitely echoboy's most im...\n",
       "                              ...                        \n",
       "4996    the result is an album that's unfortunately ba...\n",
       "4997    in the end, island makes dave sound like he's ...\n",
       "4998    beth ditto's remarkable gospel holler and ferv...\n",
       "4999    dr. john is dr. john. he's a star, and is on f...\n",
       "5000    their work, especially that displayed on refin...\n",
       "Name: body_tokens, Length: 5001, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First create a new column called \"body_tokens\"\n",
    "# transform to lowercase by applying x.lower() OR the string function str.lower()\n",
    "df['body_tokens'] = df['body'].apply(lambda x: x.lower())\n",
    "\n",
    "# view output\n",
    "df['body_tokens'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [while, for, baltimore, proves, they, can, sti...\n",
      "1       [there, 's, nothing, fake, about, the, purgato...\n",
      "2       [all, life, 's, disastrous, lows, are, here, o...\n",
      "3       [with, doris, ,, odd, future, ’, s, odysseus, ...\n",
      "4       [though, giraffe, is, definitely, echoboy, 's,...\n",
      "                              ...                        \n",
      "4996    [the, result, is, an, album, that, 's, unfortu...\n",
      "4997    [in, the, end, ,, island, makes, dave, sound, ...\n",
      "4998    [beth, ditto, 's, remarkable, gospel, holler, ...\n",
      "4999    [dr., john, is, dr., john, ., he, 's, a, star,...\n",
      "5000    [their, work, ,, especially, that, displayed, ...\n",
      "Name: body_tokens, Length: 5001, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#tokenize\n",
    "df['body_tokens'] = df['body_tokens'].apply(word_tokenize)\n",
    "\n",
    "#view output\n",
    "print(df['body_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [while, for, baltimore, proves, they, can, sti...\n",
      "1       [there, 's, nothing, fake, about, the, purgato...\n",
      "2       [all, life, 's, disastrous, lows, are, here, o...\n",
      "3       [with, doris, odd, future, ’, s, odysseus, is,...\n",
      "4       [though, giraffe, is, definitely, echoboy, 's,...\n",
      "                              ...                        \n",
      "4996    [the, result, is, an, album, that, 's, unfortu...\n",
      "4997    [in, the, end, island, makes, dave, sound, lik...\n",
      "4998    [beth, ditto, 's, remarkable, gospel, holler, ...\n",
      "4999    [dr., john, is, dr., john, he, 's, a, star, an...\n",
      "5000    [their, work, especially, that, displayed, on,...\n",
      "Name: body_tokens, Length: 5001, dtype: object\n"
     ]
    }
   ],
   "source": [
    "punctuations = list(string.punctuation)\n",
    "\n",
    "#remove punctuation. Note the list comprehension used with lambda x\n",
    "df['body_tokens'] = df['body_tokens'].apply(lambda token: [char for char in token if char not in punctuations])\n",
    "\n",
    "#view output\n",
    "print(df['body_tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            body_tokens  token_count\n",
      "0     [while, for, baltimore, proves, they, can, sti...           38\n",
      "1     [there, 's, nothing, fake, about, the, purgato...           28\n",
      "2     [all, life, 's, disastrous, lows, are, here, o...           13\n",
      "3     [with, doris, odd, future, ’, s, odysseus, is,...           18\n",
      "4     [though, giraffe, is, definitely, echoboy, 's,...           51\n",
      "...                                                 ...          ...\n",
      "4996  [the, result, is, an, album, that, 's, unfortu...           27\n",
      "4997  [in, the, end, island, makes, dave, sound, lik...           17\n",
      "4998  [beth, ditto, 's, remarkable, gospel, holler, ...           25\n",
      "4999  [dr., john, is, dr., john, he, 's, a, star, an...           18\n",
      "5000  [their, work, especially, that, displayed, on,...           28\n",
      "\n",
      "[5001 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Get total token count for each row:\n",
    "df['token_count'] = df['body_tokens'].apply(lambda x: len(x))\n",
    "\n",
    "print(df[['body_tokens','token_count']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reflection:**<br>\n",
    "There are many other preprocessing steps, including stopword removal, normalizing text (removing URLs and hashtags), stripping whitespace, counting word frequencies, and removing infrequent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2. Creating dictionary counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_sent = open(\"../day-2/data/positive_words.txt\", encoding='utf-8').read()\n",
    "neg_sent = open(\"../day-2/data/negative_words.txt\", encoding='utf-8').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remember the split function? We'll split on the newline character (\\n) to create a list\n",
    "positive_words=pos_sent.split('\\n')\n",
    "negative_words=neg_sent.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2231\n",
      "3906\n"
     ]
    }
   ],
   "source": [
    "#count number of words in each list\n",
    "print(len(positive_words))\n",
    "print(len(negative_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge - SOLUTION\n",
    "1. Create a column with the number of positive words, and another with the proportion of positive words\n",
    "2. Create a column with the number of negative words, and another with the proportion of negative words\n",
    "3. Print the average proportion of negative and positive words by genre\n",
    "4. Compare this to the average score by genre\n",
    "\n",
    "*Note:* You won't be able to do this challenge (or anything else in this section) if you didn't complete the first challenge above to preprocess `df['body']` into `df['body_tokens']`. If you skipped that part or got stuck, copy and run the solution from `solutions/dictionary-methods-solutions.ipynb` before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best way to do this: A list comprehension in an apply statement!\n",
    "df['pos_num'] = df['body_tokens'].apply(lambda x: len([word for word in x if word in positive_words]))\n",
    "df['neg_num'] = df['body_tokens'].apply(lambda x: len([word for word in x if word in negative_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Another way: Create functions to count matches!\n",
    "# Not really necessary for a case this simple, but easy to modify for more complex cases.\n",
    "\n",
    "def count_pos_words(tokens):\n",
    "    '''\n",
    "    Counts number of positive words in a preprocessed (tokenized, etc.) text, \n",
    "    using already-defined positive sentiment dictionary `positive_words`. \n",
    "    \n",
    "    Args:\n",
    "        tokens: preprocessed, tokenized input text\n",
    "        positive_words: global var, list of positive sentiment words (sentiment dictionary)\n",
    "    Returns:\n",
    "        count: number of times any word from positive_words occurs in tokens (input text)\n",
    "    '''\n",
    "    \n",
    "    global positive_words\n",
    "    \n",
    "    count = 0 # initialize counter\n",
    "    \n",
    "    # Loop over input text; if word occurs in dictionary, add 1 to count\n",
    "    for word in tokens:\n",
    "        if word in positive_words:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "    \n",
    "def count_neg_words(tokens):\n",
    "    '''\n",
    "    Counts number of negative words in a preprocessed (tokenized, etc.) text, \n",
    "    using already-defined negative sentiment dictionary `negative_words`.\n",
    "    \n",
    "    Args:\n",
    "        tokens: preprocessed, tokenized input text\n",
    "        negative_words: global var, list of negative sentiment words (sentiment dictionary)\n",
    "    Returns:\n",
    "        count: number of times any word from negative_words occurs in tokens (input text)\n",
    "    '''\n",
    "    \n",
    "    global negative_words\n",
    "    \n",
    "    count = 0 # initialize counter\n",
    "    \n",
    "    # Loop over input text; if word occurs in dictionary, add 1 to count\n",
    "    for word in tokens:\n",
    "        if word in negative_words:\n",
    "            count += 1\n",
    "    \n",
    "    return count\n",
    "\n",
    "# Use the functions via df[col].apply()\n",
    "df['pos_num'] = df['body_tokens'].apply(count_pos_words)\n",
    "df['neg_num'] = df['body_tokens'].apply(count_neg_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>album</th>\n",
       "      <th>artist</th>\n",
       "      <th>genre</th>\n",
       "      <th>critic</th>\n",
       "      <th>score</th>\n",
       "      <th>body</th>\n",
       "      <th>body_tokens</th>\n",
       "      <th>token_count</th>\n",
       "      <th>pos_num</th>\n",
       "      <th>neg_num</th>\n",
       "      <th>pos_prop</th>\n",
       "      <th>neg_prop</th>\n",
       "      <th>pos_lean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Don't Panic</td>\n",
       "      <td>All Time Low</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>Kerrang!</td>\n",
       "      <td>74.0</td>\n",
       "      <td>While For Baltimore proves they can still writ...</td>\n",
       "      <td>[while, for, baltimore, proves, they, can, sti...</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.026316</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.026316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fear and Saturday Night</td>\n",
       "      <td>Ryan Bingham</td>\n",
       "      <td>Country</td>\n",
       "      <td>Uncut</td>\n",
       "      <td>70.0</td>\n",
       "      <td>There's nothing fake about the purgatorial nar...</td>\n",
       "      <td>[there, 's, nothing, fake, about, the, purgato...</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>-0.107143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Way I'm Livin'</td>\n",
       "      <td>Lee Ann Womack</td>\n",
       "      <td>Country</td>\n",
       "      <td>Q Magazine</td>\n",
       "      <td>84.0</td>\n",
       "      <td>All life's disastrous lows are here on a caree...</td>\n",
       "      <td>[all, life, 's, disastrous, lows, are, here, o...</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>-0.076923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doris</td>\n",
       "      <td>Earl Sweatshirt</td>\n",
       "      <td>Rap</td>\n",
       "      <td>Pitchfork</td>\n",
       "      <td>82.0</td>\n",
       "      <td>With Doris, Odd Future’s Odysseus is finally b...</td>\n",
       "      <td>[with, doris, odd, future, ’, s, odysseus, is,...</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>-0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Giraffe</td>\n",
       "      <td>Echoboy</td>\n",
       "      <td>Rock</td>\n",
       "      <td>AllMusic</td>\n",
       "      <td>71.0</td>\n",
       "      <td>Though Giraffe is definitely Echoboy's most im...</td>\n",
       "      <td>[though, giraffe, is, definitely, echoboy, 's,...</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>-0.039216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Outer South</td>\n",
       "      <td>Conor Oberst And The Mystic Valley Band</td>\n",
       "      <td>Indie</td>\n",
       "      <td>Slant Magazine</td>\n",
       "      <td>67.0</td>\n",
       "      <td>The result is an album that's unfortunately ba...</td>\n",
       "      <td>[the, result, is, an, album, that, 's, unfortu...</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>-0.111111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>On An Island</td>\n",
       "      <td>David Gilmour</td>\n",
       "      <td>Rock</td>\n",
       "      <td>E! Online</td>\n",
       "      <td>67.0</td>\n",
       "      <td>In the end, Island makes Dave sound like he's ...</td>\n",
       "      <td>[in, the, end, island, makes, dave, sound, lik...</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Movement</td>\n",
       "      <td>Gossip</td>\n",
       "      <td>Indie</td>\n",
       "      <td>Uncut</td>\n",
       "      <td>81.0</td>\n",
       "      <td>Beth Ditto's remarkable gospel holler and ferv...</td>\n",
       "      <td>[beth, ditto, 's, remarkable, gospel, holler, ...</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.080000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Locked Down</td>\n",
       "      <td>Dr. John</td>\n",
       "      <td>Pop/Rock</td>\n",
       "      <td>PopMatters</td>\n",
       "      <td>86.0</td>\n",
       "      <td>Dr. John is Dr. John. He's a star, and is on f...</td>\n",
       "      <td>[dr., john, is, dr., john, he, 's, a, star, an...</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>And Their Refinement Of The Decline</td>\n",
       "      <td>Stars Of The Lid</td>\n",
       "      <td>Rock</td>\n",
       "      <td>PopMatters</td>\n",
       "      <td>87.0</td>\n",
       "      <td>Their work, especially that displayed on Refin...</td>\n",
       "      <td>[their, work, especially, that, displayed, on,...</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.178571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.178571</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5001 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    album  \\\n",
       "0                             Don't Panic   \n",
       "1                 Fear and Saturday Night   \n",
       "2                      The Way I'm Livin'   \n",
       "3                                   Doris   \n",
       "4                                 Giraffe   \n",
       "...                                   ...   \n",
       "4996                          Outer South   \n",
       "4997                         On An Island   \n",
       "4998                             Movement   \n",
       "4999                          Locked Down   \n",
       "5000  And Their Refinement Of The Decline   \n",
       "\n",
       "                                       artist     genre          critic  \\\n",
       "0                                All Time Low  Pop/Rock        Kerrang!   \n",
       "1                                Ryan Bingham   Country           Uncut   \n",
       "2                              Lee Ann Womack   Country      Q Magazine   \n",
       "3                             Earl Sweatshirt       Rap       Pitchfork   \n",
       "4                                     Echoboy      Rock        AllMusic   \n",
       "...                                       ...       ...             ...   \n",
       "4996  Conor Oberst And The Mystic Valley Band     Indie  Slant Magazine   \n",
       "4997                            David Gilmour      Rock       E! Online   \n",
       "4998                                   Gossip     Indie           Uncut   \n",
       "4999                                 Dr. John  Pop/Rock      PopMatters   \n",
       "5000                         Stars Of The Lid      Rock      PopMatters   \n",
       "\n",
       "      score                                               body  \\\n",
       "0      74.0  While For Baltimore proves they can still writ...   \n",
       "1      70.0  There's nothing fake about the purgatorial nar...   \n",
       "2      84.0  All life's disastrous lows are here on a caree...   \n",
       "3      82.0  With Doris, Odd Future’s Odysseus is finally b...   \n",
       "4      71.0  Though Giraffe is definitely Echoboy's most im...   \n",
       "...     ...                                                ...   \n",
       "4996   67.0  The result is an album that's unfortunately ba...   \n",
       "4997   67.0  In the end, Island makes Dave sound like he's ...   \n",
       "4998   81.0  Beth Ditto's remarkable gospel holler and ferv...   \n",
       "4999   86.0  Dr. John is Dr. John. He's a star, and is on f...   \n",
       "5000   87.0  Their work, especially that displayed on Refin...   \n",
       "\n",
       "                                            body_tokens  token_count  pos_num  \\\n",
       "0     [while, for, baltimore, proves, they, can, sti...           38        1   \n",
       "1     [there, 's, nothing, fake, about, the, purgato...           28        0   \n",
       "2     [all, life, 's, disastrous, lows, are, here, o...           13        0   \n",
       "3     [with, doris, odd, future, ’, s, odysseus, is,...           18        0   \n",
       "4     [though, giraffe, is, definitely, echoboy, 's,...           51        2   \n",
       "...                                                 ...          ...      ...   \n",
       "4996  [the, result, is, an, album, that, 's, unfortu...           27        0   \n",
       "4997  [in, the, end, island, makes, dave, sound, lik...           17        3   \n",
       "4998  [beth, ditto, 's, remarkable, gospel, holler, ...           25        2   \n",
       "4999  [dr., john, is, dr., john, he, 's, a, star, an...           18        1   \n",
       "5000  [their, work, especially, that, displayed, on,...           28        5   \n",
       "\n",
       "      neg_num  pos_prop  neg_prop  pos_lean  \n",
       "0           0  0.026316  0.000000  0.026316  \n",
       "1           3  0.000000  0.107143 -0.107143  \n",
       "2           1  0.000000  0.076923 -0.076923  \n",
       "3           1  0.000000  0.055556 -0.055556  \n",
       "4           4  0.039216  0.078431 -0.039216  \n",
       "...       ...       ...       ...       ...  \n",
       "4996        3  0.000000  0.111111 -0.111111  \n",
       "4997        0  0.176471  0.000000  0.176471  \n",
       "4998        0  0.080000  0.000000  0.080000  \n",
       "4999        0  0.055556  0.000000  0.055556  \n",
       "5000        0  0.178571  0.000000  0.178571  \n",
       "\n",
       "[5001 rows x 13 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pos_prop'] = df['pos_num']/df['token_count']\n",
    "df['neg_prop'] = df['neg_num']/df['token_count']\n",
    "df['pos_lean'] = df['pos_prop']-df['neg_prop']\n",
    "df.drop('release_date', axis=1, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "Folk                      0.073189\n",
       "Jazz                      0.060588\n",
       "R&B;                      0.050918\n",
       "Indie                     0.048024\n",
       "Alternative/Indie Rock    0.046710\n",
       "Rock                      0.044023\n",
       "Electronic                0.043299\n",
       "Pop/Rock                  0.042071\n",
       "Pop                       0.040085\n",
       "Country                   0.039038\n",
       "Dance                     0.037849\n",
       "Rap                       0.036232\n",
       "Name: pos_lean, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped = df.groupby(by = 'genre')\n",
    "grouped['pos_lean'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genre\n",
       "Jazz                      77.631579\n",
       "Folk                      75.900000\n",
       "Indie                     74.400897\n",
       "Country                   74.071429\n",
       "Alternative/Indie Rock    73.928571\n",
       "Electronic                73.140351\n",
       "Pop/Rock                  73.033782\n",
       "R&B;                      72.366071\n",
       "Rap                       72.173554\n",
       "Rock                      70.754292\n",
       "Dance                     70.146341\n",
       "Pop                       64.608054\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped['score'].mean().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average sentiment and score for Pop/Rock:\n",
      " pos 0.0776, neg 0.0356, score 73.0\n",
      "\n",
      "Average sentiment and score for Country:\n",
      " pos 0.0721, neg 0.0331, score 74.1\n",
      "\n",
      "Average sentiment and score for Rap:\n",
      " pos 0.071, neg 0.0347, score 72.2\n",
      "\n",
      "Average sentiment and score for Rock:\n",
      " pos 0.0802, neg 0.0362, score 70.8\n",
      "\n",
      "Average sentiment and score for Indie:\n",
      " pos 0.0853, neg 0.0373, score 74.4\n",
      "\n",
      "Average sentiment and score for Electronic:\n",
      " pos 0.0786, neg 0.0353, score 73.1\n",
      "\n",
      "Average sentiment and score for Pop:\n",
      " pos 0.0697, neg 0.0296, score 64.6\n",
      "\n",
      "Average sentiment and score for Folk:\n",
      " pos 0.0965, neg 0.0233, score 75.9\n",
      "\n",
      "Average sentiment and score for R&B;:\n",
      " pos 0.0745, neg 0.0236, score 72.4\n",
      "\n",
      "Average sentiment and score for Alternative/Indie Rock:\n",
      " pos 0.0806, neg 0.0339, score 73.9\n",
      "\n",
      "Average sentiment and score for Dance:\n",
      " pos 0.0781, neg 0.0402, score 70.1\n",
      "\n",
      "Average sentiment and score for Jazz:\n",
      " pos 0.0873, neg 0.0267, score 77.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "genre_list = list(df['genre'].unique())\n",
    "\n",
    "for genre in genre_list:\n",
    "    pos_mean = df[df['genre']==genre]['pos_prop'].mean()\n",
    "    neg_mean = df[df['genre']==genre]['neg_prop'].mean()\n",
    "    score_mean = df[df['genre']==genre]['score'].mean()\n",
    "    print(\"Average sentiment and score for\", str(genre) + \":\\n\", \n",
    "          \"pos\", str(round(pos_mean, 4)) + \", neg\", str(round(neg_mean, 4)) + \", score\", str(round(score_mean, 1)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.      , 0.076243],\n",
       "       [0.076243, 1.      ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check out correlation (turns out to be rather weak):\n",
    "import numpy as np\n",
    "np.corrcoef(df['pos_lean'], df['score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3. Sentiment analysis using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the function CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "countvec = CountVectorizer()\n",
    "\n",
    "#create our document term matrix as a pandas dataframe\n",
    "dtm_df = pd.DataFrame(countvec.fit_transform(df.body).toarray(), columns=countvec.get_feature_names(), index = df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a columns variable that is a list of all column names\n",
    "columns = list(dtm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new variable that contains only column names that are in our postive words list\n",
    "pos_columns = [word for word in columns if word in positive_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dtm from our dtm_df that keeps only positive sentiment columns\n",
    "dtm_pos = dtm_df[pos_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-22-6c8dce3ade1d>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dtm_pos['pos_count'] = dtm_pos.sum(axis=1)\n"
     ]
    }
   ],
   "source": [
    "#count the number of positive words for each document\n",
    "dtm_pos['pos_count'] = dtm_pos.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge - SOLUTION\n",
    "1. Do the same for negative words.  \n",
    "2. Calculate the proportion of negative and positive words for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-23-89fddec5e8f7>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dtm_neg['neg_count'] = dtm_neg.sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0\n",
       "1       3\n",
       "2       1\n",
       "3       1\n",
       "4       4\n",
       "       ..\n",
       "4996    3\n",
       "4997    0\n",
       "4998    0\n",
       "4999    0\n",
       "5000    0\n",
       "Name: neg_count, Length: 5001, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a new variable that contains only column names that are in our negative words list\n",
    "neg_columns = [word for word in columns if word in negative_words]\n",
    "\n",
    "#create a dtm from our dtm_df that keeps only negative sentiment columns\n",
    "dtm_neg = dtm_df[neg_columns]\n",
    "\n",
    "#count the number of negative words for each document\n",
    "dtm_neg['neg_count'] = dtm_neg.sum(axis=1)\n",
    "\n",
    "# Check out results\n",
    "dtm_neg['neg_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-b2ef9329c295>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dtm_pos['pos_proportion'] = dtm_pos['pos_count']/dtm_df.sum(axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       0.030303\n",
      "1       0.000000\n",
      "2       0.000000\n",
      "3       0.000000\n",
      "4       0.046512\n",
      "          ...   \n",
      "4996    0.000000\n",
      "4997    0.187500\n",
      "4998    0.095238\n",
      "4999    0.062500\n",
      "5000    0.178571\n",
      "Name: pos_proportion, Length: 5001, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-24-b2ef9329c295>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dtm_neg['neg_proportion'] = dtm_neg['neg_count']/dtm_df.sum(axis=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       0.026316\n",
       "1       0.000000\n",
       "2       0.000000\n",
       "3       0.000000\n",
       "4       0.039216\n",
       "          ...   \n",
       "4996    0.000000\n",
       "4997    0.176471\n",
       "4998    0.080000\n",
       "4999    0.055556\n",
       "5000    0.178571\n",
       "Name: pos_prop, Length: 5001, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute ratio\n",
    "dtm_pos['pos_proportion'] = dtm_pos['pos_count']/dtm_df.sum(axis=1)\n",
    "dtm_neg['neg_proportion'] = dtm_neg['neg_count']/dtm_df.sum(axis=1)\n",
    "\n",
    "# Compare manual version with scikit learn CountVectorizer results:\n",
    "print(dtm_pos['pos_proportion'])\n",
    "df['pos_prop']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Weighting dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Read concreteness score dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_score = pd.read_csv('../day-2/data/Concreteness_ratings_Brysbaert_et_al.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Merging a DTM with a weighted dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "#open and read the novels, save them as variables\n",
    "austen_string = open('../day-2/data/Austen_PrideAndPrejudice.txt', encoding='utf-8').read()\n",
    "alcott_string = open('../day-2/data/Alcott_GarlandForGirls.txt', encoding='utf-8').read()\n",
    "\n",
    "#append each novel to the list\n",
    "text_list.append(austen_string)\n",
    "text_list.append(alcott_string)\n",
    "\n",
    "countvec = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "novels_df = pd.DataFrame(countvec.fit_transform(text_list).toarray(), columns=countvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=list(novels_df)\n",
    "columns_con = [word for word in columns if word in list(con_score['Word'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "novels_df_con = novels_df[columns_con]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = novels_df_con.transpose()\n",
    "df.rename(columns={0: 'Austen', 1: 'Alcott'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rename the index 'Word', and reset the index, so the words become a column in our dataframe and we get a new index.\n",
    "df.index.names = ['Word']\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge with our dictionary dataframe, called 'con_score'\n",
    "df = df.merge(con_score, on = 'Word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Weighting term frequencies by the concreteness score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['austen_con_score'] = df['Austen'] * df['Conc.M']\n",
    "df['alcott_con_score'] = df['Alcott'] * df['Conc.M']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge - SOLUTION\n",
    "\n",
    "Calculate and print the average concreteness score for each text. Careful! Think through this before you implement it. You want the average score, normalized over all the words in the text. \n",
    "\n",
    "*Hint:* Think about these two dataframes you have in memory: `df`, which has concreteness-scored words and their counts per novel; and `novels_df`, which has all words per novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Concreteness for Austen's 'Pride and Prejudice'\n",
      "2.783289058278108\n",
      "\n",
      "Mean Concreteness for Alcott's 'A Garland for Girls'\n",
      "3.1534507874015745\n"
     ]
    }
   ],
   "source": [
    "# We'll devide the sum of the concreteness score by the total word count for each novel\n",
    "print(\"Mean Concreteness for Austen's 'Pride and Prejudice'\")\n",
    "print(df['austen_con_score'].sum()/df['Austen'].sum())\n",
    "print()\n",
    "print(\"Mean Concreteness for Alcott's 'A Garland for Girls'\")\n",
    "print(df['alcott_con_score'].sum()/df['Alcott'].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge - SOLUTION\n",
    "Print the most concrete and abstract terms in Austen and in Alcott. Don't worry about term frequencies; just look at the raw score of words present in each novel.<br>\n",
    "*Hint:* You can't simply sort on the column `austen_con_score` and so on. Why not? What are your next steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Conc.M</th>\n",
       "      <th>Alcott</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>house</td>\n",
       "      <td>5.00</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6033</th>\n",
       "      <td>water</td>\n",
       "      <td>5.00</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>bed</td>\n",
       "      <td>5.00</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>boots</td>\n",
       "      <td>5.00</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2139</th>\n",
       "      <td>fish</td>\n",
       "      <td>5.00</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1891</th>\n",
       "      <td>especially</td>\n",
       "      <td>1.28</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5094</th>\n",
       "      <td>somewhat</td>\n",
       "      <td>1.28</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4705</th>\n",
       "      <td>sanctimonious</td>\n",
       "      <td>1.28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>hope</td>\n",
       "      <td>1.25</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6080</th>\n",
       "      <td>whatsoever</td>\n",
       "      <td>1.17</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4262 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Word  Conc.M  Alcott\n",
       "2692          house    5.00      65\n",
       "6033          water    5.00      32\n",
       "470             bed    5.00      25\n",
       "590           boots    5.00      17\n",
       "2139           fish    5.00      17\n",
       "...             ...     ...     ...\n",
       "1891     especially    1.28      12\n",
       "5094       somewhat    1.28       5\n",
       "4705  sanctimonious    1.28       1\n",
       "2671           hope    1.25      40\n",
       "6080     whatsoever    1.17       1\n",
       "\n",
       "[4262 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a new dataframe that keeps only words that have a non-zero value in Alcott\n",
    "df_alcott = df[df['Alcott']>0]\n",
    "#Sort on 'Conc.M' and print in descending order for most concrete words\n",
    "df_alcott[['Word', 'Conc.M', 'Alcott']].sort_values(by=['Conc.M', 'Alcott'], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Conc.M</th>\n",
       "      <th>Austen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2692</th>\n",
       "      <td>house</td>\n",
       "      <td>5.00</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>ball</td>\n",
       "      <td>5.00</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5198</th>\n",
       "      <td>stairs</td>\n",
       "      <td>5.00</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>bed</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>921</th>\n",
       "      <td>clock</td>\n",
       "      <td>5.00</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>hope</td>\n",
       "      <td>1.25</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>absurdity</td>\n",
       "      <td>1.25</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>advantageously</td>\n",
       "      <td>1.24</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2873</th>\n",
       "      <td>infinitely</td>\n",
       "      <td>1.22</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>490</th>\n",
       "      <td>belief</td>\n",
       "      <td>1.19</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4070 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Word  Conc.M  Austen\n",
       "2692           house    5.00     108\n",
       "413             ball    5.00      36\n",
       "5198          stairs    5.00      24\n",
       "470              bed    5.00       6\n",
       "921            clock    5.00       6\n",
       "...              ...     ...     ...\n",
       "2671            hope    1.25     121\n",
       "23         absurdity    1.25       1\n",
       "109   advantageously    1.24       2\n",
       "2873      infinitely    1.22       4\n",
       "490           belief    1.19      15\n",
       "\n",
       "[4070 rows x 3 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a new dataframe that keeps only words that have a non-zero value in Austen\n",
    "df_austen = df[df['Austen']>0]\n",
    "df_austen[['Word', 'Conc.M', 'Austen']].sort_values(by=['Conc.M', 'Austen'], ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus: Weighting words with TF-IDF<a id='tfidf'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our Music Reviews corpus for this. Read into Pandas DataFrame:\n",
    "df = pd.read_csv(\"../day-2/data/BDHSI2016_music_reviews.csv\", encoding='utf-8', sep = '\\t')\n",
    "\n",
    "# Clean out numbers:\n",
    "df['body'] = df['body'].apply(lambda x: ''.join([i for i in x if not i.isdigit()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aahs</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>abc</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooey</th>\n",
       "      <th>zoomer</th>\n",
       "      <th>zu</th>\n",
       "      <th>zydeco</th>\n",
       "      <th>álbum</th>\n",
       "      <th>être</th>\n",
       "      <th>über</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5001 rows × 16139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       aa  aaaa  aahs  aaliyah  aaron   ab  abandon  abandoned  abandoning  \\\n",
       "0     0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "1     0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "2     0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "3     0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "4     0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "...   ...   ...   ...      ...    ...  ...      ...        ...         ...   \n",
       "4996  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "4997  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "4998  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "4999  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "5000  0.0   0.0   0.0      0.0    0.0  0.0      0.0        0.0         0.0   \n",
       "\n",
       "      abc  ...  zone  zones  zoo  zooey  zoomer   zu  zydeco  álbum  être  \\\n",
       "0     0.0  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   \n",
       "1     0.0  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   \n",
       "2     0.0  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   \n",
       "3     0.0  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   \n",
       "4     0.0  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   \n",
       "...   ...  ...   ...    ...  ...    ...     ...  ...     ...    ...   ...   \n",
       "4996  0.0  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   \n",
       "4997  0.0  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   \n",
       "4998  0.0  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   \n",
       "4999  0.0  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   \n",
       "5000  0.0  ...   0.0    0.0  0.0    0.0     0.0  0.0     0.0    0.0   0.0   \n",
       "\n",
       "      über  \n",
       "0      0.0  \n",
       "1      0.0  \n",
       "2      0.0  \n",
       "3      0.0  \n",
       "4      0.0  \n",
       "...    ...  \n",
       "4996   0.0  \n",
       "4997   0.0  \n",
       "4998   0.0  \n",
       "4999   0.0  \n",
       "5000   0.0  \n",
       "\n",
       "[5001 rows x 16139 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import the function TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidfvec = TfidfVectorizer()\n",
    "\n",
    "#create the dtm, but with cells weigthed by the tf-idf score.\n",
    "tfidf_df = pd.DataFrame(tfidfvec.fit_transform(df['body']).toarray(), columns=tfidfvec.get_feature_names())\n",
    "\n",
    "#view results\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying Distinctive Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creat dataset with document index and genre\n",
    "df_genre = df['genre'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge this into the dtm_tfidf_df\n",
    "merged_df = df_genre.join(tfidf_df, how = 'right', lsuffix='_x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rap Words\n",
      "blank             0.854475\n",
      "waste             0.755918\n",
      "amiable           0.730963\n",
      "awesomely         0.717079\n",
      "joyless           0.687687\n",
      "beastie           0.672439\n",
      "same              0.672392\n",
      "sucker            0.663760\n",
      "vanguard          0.661978\n",
      "tight             0.653993\n",
      "lamest            0.639377\n",
      "derivativeness    0.636271\n",
      "authentic         0.627192\n",
      "diverse           0.623373\n",
      "sermon            0.621175\n",
      "pushin            0.617699\n",
      "mastermind        0.609213\n",
      "neat              0.608922\n",
      "we                0.600755\n",
      "lift              0.591821\n",
      "dtype: float64\n",
      "\n",
      "Indie Words\n",
      "underplayed    0.516717\n",
      "prisoner       0.512087\n",
      "jezabels       0.512087\n",
      "careworn       0.509386\n",
      "folk           0.509321\n",
      "fourth         0.480502\n",
      "heyday         0.469035\n",
      "their          0.458950\n",
      "riffed         0.458182\n",
      "bet            0.456164\n",
      "victory        0.449289\n",
      "exhausted      0.445969\n",
      "bigger         0.441849\n",
      "babelfished    0.431543\n",
      "lightweight    0.428857\n",
      "exercised      0.428857\n",
      "powerhouse     0.422192\n",
      "worn           0.416482\n",
      "try            0.415525\n",
      "triumph        0.413976\n",
      "dtype: float64\n",
      "\n",
      "Jazz Words\n",
      "purely        0.544477\n",
      "descending    0.519218\n",
      "devotional    0.507724\n",
      "recordings    0.499963\n",
      "languid       0.487715\n",
      "clarinet      0.479643\n",
      "amidst        0.465227\n",
      "duhnam        0.456224\n",
      "carper        0.456224\n",
      "apply         0.435261\n",
      "sidelined     0.426059\n",
      "spacier       0.426059\n",
      "holds         0.425910\n",
      "as            0.423643\n",
      "enjoyable     0.419141\n",
      "mose          0.414479\n",
      "allison       0.414479\n",
      "her           0.413922\n",
      "soared        0.413445\n",
      "slowly        0.412241\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#pull out the reviews for three genres, Rap, Alternative/Indie Rock, and Jazz\n",
    "dtm_rap = merged_df[merged_df['genre_x']=='Rap']\n",
    "dtm_indie = merged_df[merged_df['genre_x']=='Alternative/Indie Rock']\n",
    "dtm_jazz = merged_df[merged_df['genre_x']=='Jazz']\n",
    "\n",
    "#print the words with the highest TF-IDF scores for each genre\n",
    "print('Rap Words')\n",
    "print(dtm_rap.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print('Indie Words')\n",
    "print(dtm_indie.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print('Jazz Words')\n",
    "print(dtm_jazz.max(numeric_only=True).sort_values(ascending=False)[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge - SOLUTION\n",
    "\n",
    "Compare the distinctive words for two artists in the data.\n",
    "\n",
    "Note: the artists should have a number of reviews, so check your frequency counts to identify artists.\n",
    "\n",
    "*Hint:* Copy and paste the above code and modify it as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0             All Time Low\n",
      "250        Fujiya & Miyagi\n",
      "500                 Katy B\n",
      "750            Negativland\n",
      "1000                Tricky\n",
      "1250        Emmylou Harris\n",
      "1500                Shamir\n",
      "1750              Thursday\n",
      "2000                 Psapp\n",
      "2250             Cornelius\n",
      "2500                Vessel\n",
      "2750                Clutch\n",
      "3000              Ice Cube\n",
      "3250              Autechre\n",
      "3500            Kanye West\n",
      "3750              The Fall\n",
      "4000           Gaz Coombes\n",
      "4250    Noah and the Whale\n",
      "4500        Mumford & Sons\n",
      "4750            Bobby Conn\n",
      "5000      Stars Of The Lid\n",
      "Name: artist, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df['artist'][::250]) # Random look at artists to choose from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_x</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aahs</th>\n",
       "      <th>aaliyah</th>\n",
       "      <th>aaron</th>\n",
       "      <th>ab</th>\n",
       "      <th>abandon</th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abandoning</th>\n",
       "      <th>...</th>\n",
       "      <th>zone</th>\n",
       "      <th>zones</th>\n",
       "      <th>zoo</th>\n",
       "      <th>zooey</th>\n",
       "      <th>zoomer</th>\n",
       "      <th>zu</th>\n",
       "      <th>zydeco</th>\n",
       "      <th>álbum</th>\n",
       "      <th>être</th>\n",
       "      <th>über</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>All Time Low</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ryan Bingham</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lee Ann Womack</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Earl Sweatshirt</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Echoboy</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>Conor Oberst And The Mystic Valley Band</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>David Gilmour</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>Gossip</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>Dr. John</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5000</th>\n",
       "      <td>Stars Of The Lid</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5001 rows × 16140 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     artist_x   aa  aaaa  aahs  aaliyah  \\\n",
       "0                                All Time Low  0.0   0.0   0.0      0.0   \n",
       "1                                Ryan Bingham  0.0   0.0   0.0      0.0   \n",
       "2                              Lee Ann Womack  0.0   0.0   0.0      0.0   \n",
       "3                             Earl Sweatshirt  0.0   0.0   0.0      0.0   \n",
       "4                                     Echoboy  0.0   0.0   0.0      0.0   \n",
       "...                                       ...  ...   ...   ...      ...   \n",
       "4996  Conor Oberst And The Mystic Valley Band  0.0   0.0   0.0      0.0   \n",
       "4997                            David Gilmour  0.0   0.0   0.0      0.0   \n",
       "4998                                   Gossip  0.0   0.0   0.0      0.0   \n",
       "4999                                 Dr. John  0.0   0.0   0.0      0.0   \n",
       "5000                         Stars Of The Lid  0.0   0.0   0.0      0.0   \n",
       "\n",
       "      aaron   ab  abandon  abandoned  abandoning  ...  zone  zones  zoo  \\\n",
       "0       0.0  0.0      0.0        0.0         0.0  ...   0.0    0.0  0.0   \n",
       "1       0.0  0.0      0.0        0.0         0.0  ...   0.0    0.0  0.0   \n",
       "2       0.0  0.0      0.0        0.0         0.0  ...   0.0    0.0  0.0   \n",
       "3       0.0  0.0      0.0        0.0         0.0  ...   0.0    0.0  0.0   \n",
       "4       0.0  0.0      0.0        0.0         0.0  ...   0.0    0.0  0.0   \n",
       "...     ...  ...      ...        ...         ...  ...   ...    ...  ...   \n",
       "4996    0.0  0.0      0.0        0.0         0.0  ...   0.0    0.0  0.0   \n",
       "4997    0.0  0.0      0.0        0.0         0.0  ...   0.0    0.0  0.0   \n",
       "4998    0.0  0.0      0.0        0.0         0.0  ...   0.0    0.0  0.0   \n",
       "4999    0.0  0.0      0.0        0.0         0.0  ...   0.0    0.0  0.0   \n",
       "5000    0.0  0.0      0.0        0.0         0.0  ...   0.0    0.0  0.0   \n",
       "\n",
       "      zooey  zoomer   zu  zydeco  álbum  être  über  \n",
       "0       0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "1       0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "2       0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "3       0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "4       0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "...     ...     ...  ...     ...    ...   ...   ...  \n",
       "4996    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "4997    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "4998    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "4999    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "5000    0.0     0.0  0.0     0.0    0.0   0.0   0.0  \n",
       "\n",
       "[5001 rows x 16140 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#creat dataset with document index and artist, merge\n",
    "merged_df_artist = df['artist'].to_frame()\n",
    "merged_df_artist = merged_df_artist.join(tfidf_df, how = 'right', lsuffix='_x')\n",
    "\n",
    "#view result\n",
    "merged_df_artist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most distinctive words for R.E.M.\n",
      "reliably        0.579442\n",
      "staid           0.550549\n",
      "every           0.530261\n",
      "isn             0.523994\n",
      "unfussy         0.513744\n",
      "crucially       0.459618\n",
      "committed       0.434459\n",
      "convincing      0.434459\n",
      "fast            0.424265\n",
      "collapse        0.421777\n",
      "habit           0.410508\n",
      "accelerate      0.410508\n",
      "stun            0.391646\n",
      "forming         0.391646\n",
      "dec             0.376505\n",
      "noncommittal    0.368986\n",
      "beautiful       0.358367\n",
      "mostly          0.352703\n",
      "stutter         0.352486\n",
      "stipe           0.352032\n",
      "dtype: float64\n",
      "\n",
      "Most distinctive words for Arcade Fire\n",
      "disc           0.459815\n",
      "reflektor      0.431429\n",
      "jumping        0.423503\n",
      "patterns       0.409032\n",
      "features       0.408639\n",
      "bitterness     0.408519\n",
      "shorter        0.397541\n",
      "radiates       0.389749\n",
      "affection      0.389749\n",
      "suburbs        0.377718\n",
      "beguiling      0.374164\n",
      "detox          0.373836\n",
      "components     0.364664\n",
      "divergence     0.363223\n",
      "redeem         0.356659\n",
      "paced          0.352743\n",
      "letter         0.350524\n",
      "divergent      0.345035\n",
      "double         0.336293\n",
      "proposition    0.336020\n",
      "dtype: float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define artists to identify distinctive words for\n",
    "artist1 = 'R.E.M.'\n",
    "artist2 = 'Arcade Fire'\n",
    "\n",
    "# Filter merged_df_artist to these two artists\n",
    "dtm1 = merged_df_artist[merged_df_artist['artist_x']==artist1]\n",
    "dtm2 = merged_df_artist[merged_df_artist['artist_x']==artist2]\n",
    "\n",
    "# Display distinctive words\n",
    "print(\"Most distinctive words for \" + str(artist1))\n",
    "print(dtm1.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()\n",
    "print(\"Most distinctive words for \" + str(artist2))\n",
    "print(dtm2.max(numeric_only=True).sort_values(ascending=False)[0:20])\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
